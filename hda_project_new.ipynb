{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bed Posture and Subject Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 01:15:03.944105: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 01:15:04.926026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Data Load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.layers import Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import ndimage\n",
    "#import skimage.transform\n",
    "\n",
    "# Other\n",
    "import math\n",
    "import h5py\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Height/cm</th>\n",
       "      <th>Weight/kg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subject-Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>183</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>183</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>177</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>172</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>169</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>179</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>186</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>174</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>176</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33</td>\n",
       "      <td>170</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age  Height/cm  Weight/kg\n",
       "Subject-Number                           \n",
       "1                19        175         87\n",
       "2                23        183         85\n",
       "3                23        183        100\n",
       "4                24        177         70\n",
       "5                24        172         66\n",
       "6                26        169         83\n",
       "7                27        179         96\n",
       "8                27        186         63\n",
       "9                30        174         74\n",
       "10               30        174         79\n",
       "11               30        176         91\n",
       "12               33        170         78\n",
       "13               34        174         74"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "subj_inf = pd.read_csv(path + '/data/experiment-i/subject-info-i.csv', encoding = 'utf_16', index_col='Subject-Number')\n",
    "display(subj_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subjects in Experiment I are: ['S9', 'S10', 'S12', 'S7', 'S6', 'S4', 'S1', 'S8', 'S11', 'S3', 'S13', 'S2', 'S5']\n"
     ]
    }
   ],
   "source": [
    "# 13 subjects in Experiment I\n",
    "subj_list = [file for file in os.listdir(path +'/data/experiment-i/') if os.path.isdir(os.path.join(path +'/data/experiment-i/', file))]\n",
    "print(f\"The subjects in Experiment I are: {subj_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positions for each subject are: 17\n"
     ]
    }
   ],
   "source": [
    "positions_cnt = [file for file in os.listdir(path +'/data/experiment-i/S5/') if os.path.isfile(os.path.join(path +'/data/experiment-i/S5/', file))]\n",
    "print(f\"The number of positions for each subject are: {len(positions_cnt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position vector\n",
    "positions_i = [\"placeholder\", \"supine\", \"right\",\n",
    "                     \"left\", \"right\", \"right\",\n",
    "                     \"left\", \"left\", \"supine\",\n",
    "                     \"supine\", \"supine\", \"supine\",\n",
    "                     \"supine\", \"right\", \"left\",\n",
    "                     \"supine\", \"supine\", \"supine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use position names to define 3 common positions\n",
    "\n",
    "def token_position(x):\n",
    "    return {\n",
    "        'supine': 0,\n",
    "        'left': 1,\n",
    "        'right': 2,\n",
    "        'left_fetus': 1,\n",
    "        'right_fetus': 2\n",
    "    }[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 2048)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect one file\n",
    "test_read = np.loadtxt(path + '/data/experiment-i/S5/1.txt')\n",
    "test_read.shape # (101, 2048) = (number of samples, number of sensors (32x64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Test read - Experiment I, Subject 5, Position 1\")\n",
    "fig = plt.imshow(test_read[20,:].reshape(64, 32)) # 20th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first two measurements in each file are corrupted\n",
    "fig, axes = plt.subplots(figsize=(10,6), ncols=3)\n",
    "fig.suptitle(\"Corrupted images - Experiment I\")\n",
    "for ii in range(3):\n",
    "    ax = axes[ii]\n",
    "    ax.imshow(test_read[ii,:].reshape(64,32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['S9', 'S10', 'S12', 'S7', 'S6', 'S4', 'S1', 'S8', 'S11', 'S3', 'S13', 'S2', 'S5'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all the data into a dictionary, where the keys are the subjects and \n",
    "# values: images (index 0) and their position's number (index 1)\n",
    "\n",
    "exp_i_data = {}\n",
    "\n",
    "for _, dirs, _ in os.walk(path + '/data/experiment-i/'): # root, directories, files: indices for os.walk    \n",
    "    for dir in dirs:\n",
    "        subject = dir\n",
    "        data = None\n",
    "        labels = None\n",
    "        for _, _, files in os.walk(path + '/data/experiment-i/' + dir):\n",
    "            for file in files:\n",
    "                file_path = path + '/data/experiment-i/' + dir + '/' + file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    # remove first and last 3 frames of each sequence\n",
    "                    for line in f.read().splitlines()[3:-3]:\n",
    "                        raw_data = np.fromstring(line, dtype=float, sep='\\t').reshape(1, 64, 32)\n",
    "                        # apply median filter of 3x3\n",
    "                        raw_data = ndimage.median_filter(raw_data, size=(1,3,3))\n",
    "                        # Normalize the data: change range from [0-1000] to [0-255]\n",
    "                        file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
    "                        # Label the data using position number from file name\n",
    "                        file_label = token_position(positions_i[int(file[:-4])])                        \n",
    "                        file_label = np.array([file_label])\n",
    "\n",
    "                        if data is None:\n",
    "                            data = file_data\n",
    "                        else:\n",
    "                            # Concatenate the new data along the first axis\n",
    "                            data = np.concatenate((data, file_data), axis=0)\n",
    "                        if labels is None:\n",
    "                            labels = file_label\n",
    "                        else:\n",
    "                            labels = np.concatenate((labels, file_label), axis=0)\n",
    "                        \n",
    "        exp_i_data[subject] = {'data': data, 'labels': labels}\n",
    "\n",
    "exp_i_data.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the labels\n",
    "def token_patient(x):\n",
    "    return {'S1': 0, 'S2': 1, 'S3': 2, 'S4': 3, 'S5': 4,  'S6': 5, 'S7': 6,\n",
    "                    'S8': 7, 'S9': 8, 'S10': 9, 'S11': 10, 'S12': 11, 'S13':12}[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each experiment\n",
    "\n",
    "# Experiment I\n",
    "X = exp_i_data[subj_list[0]]['data']\n",
    "y = exp_i_data[subj_list[0]]['labels']\n",
    "label_s = np.full(len(X), token_patient(subj_list[0]))\n",
    "\n",
    "for subject in subj_list[1:]:\n",
    "    X = np.append(X, exp_i_data[subject]['data'], axis=0)\n",
    "    y = np.append(y, exp_i_data[subject]['labels'], axis=0)\n",
    "    label_s = np.append(label_s, np.full(len(exp_i_data[subject]['data']), token_patient(subject)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment I: X shape: (18698, 64, 32), y shape: (18698,)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes of the datasets\n",
    "print(f\"Experiment I: X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "\n",
    "# 10% was used for testing and 90% for training.\n",
    "\n",
    "# Experiment I\n",
    "# indices = np.arange(len(X))\n",
    "# X_train, X_test, y_train, y_test, indices_train, indices_test, = train_test_split(X, y, indices, test_size=0.1, random_state=123)\n",
    "\n",
    "# # Split label_s accordingly using indices\n",
    "# label_s_train = label_s[indices_train]\n",
    "# label_s_test = label_s[indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_s_train = tf.keras.utils.to_categorical(label_s_train, num_classes=13)\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment I: Training data shape: (16828, 64, 32), Training labels shape: (16828, 3)\n",
      "Experiment I: Testing data shape: (1870, 64, 32), Testing labels shape: (1870,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the training and testing datasets\n",
    "\n",
    "# print(f\"Experiment I: Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
    "# print(f\"Experiment I: Testing data shape: {X_test.shape}, Testing labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proposed model from paper\n",
    "\n",
    "# can't use a sequential model as model has multiple outputs\n",
    "#each convolutional block was followed by an increasing dropout rate of 10%, 20%, 30%, 40%. \n",
    "\n",
    "# Define the functional model\n",
    "def create_model(regularization=0):\n",
    "\n",
    "    input = tf.keras.Input(shape=(64, 32, 1), name=\"img\")\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), strides=(1,1), padding='valid', kernel_regularizer=l2(regularization))(input)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.MaxPool2D((3,3), strides=(2, 2), padding='valid')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalMaxPool2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    subject_output = tf.keras.layers.Dense(13, activation='softmax', name=\"subject_output\")(x)\n",
    "    posture_output = tf.keras.layers.Dense(3, activation='softmax', name=\"posture_output\")(x)\n",
    "\n",
    "    base_cnn_model = tf.keras.Model(inputs=input, outputs=[subject_output,posture_output], name=\"base_cnn_model\")\n",
    "\n",
    "    return base_cnn_model\n",
    "    # From Figure 2 in paper\n",
    "\n",
    "    # Input: 32x64x1\n",
    "\n",
    "    # 4 main blocks of conv-batchnorm plus max pool for 1st 2 blocks\n",
    "    # 1st block: 30 x 62 x 32 of conv-batchnorm-maxpool-leakyrelu (32 filters applied), max pool would make each 30x 62 =10 x 20, cov would make (32-3+1)x(64-3+1) = 30 x 62\n",
    "    # 2nd block: 13 x 29 x 64 of conv-batchnorm-maxpool-leakyrelu\n",
    "    # 3rd block: 4 x 12 x 128 of conv-batchnorm-leakyrelu\n",
    "    # 4th block: 2 x 10 x 256 of conv-batchnorm-leakyrelu\n",
    "\n",
    "    # Two Dense layers of 256 units with dropout of 0.5\n",
    "    # Two output layers, multinomial logistic regressors for recognition of subjects (M units) and their sleep postures (N units)\n",
    "\n",
    "    # Loss function: multiclass cross-entropy loss function of subjects and users separately\n",
    "    # plus combined loss function of both with hyperparameter lambda (from 0 to 1)\n",
    "\n",
    "    # Conv kernels: 3x3, stride 1, padding valid\n",
    "    # Max pool: 3x3\n",
    "    # batch norm\n",
    "    # Leaky ReLU: alpha 0.2\n",
    "\n",
    "    # dense layers have dropout of 0.5\n",
    "\n",
    "    # each conv block was followed by increasing drop of .1, .2, .3, .4\n",
    "\n",
    "    # L2 reg loss with coefficient of 0.002\n",
    "    # 2 softmax regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# median filter of 3x3\n",
    "# remove first and last 3 frames of each sequence\n",
    "# threshold filtering and equalization histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions for Training\n",
    "\n",
    "### Notation\n",
    "- Let $I$ be the input pressure map.\n",
    "- Let $i$ be the example index.\n",
    "- Let $\\gamma$ and $\\delta$ be the user and the posture, respectively\n",
    "- Let M and N be the number of users and postures, respectively\n",
    "\n",
    "### Multi-Class Cross Entropy Loss Functions\n",
    "\n",
    "User Loss:\n",
    "\n",
    "$$ L_{user} = - \\sum_{j=1}^{M} \\gamma_{ij} \\log P({\\gamma_{j}}|I_i)$$\n",
    "\n",
    "\n",
    "Posture Loss:\n",
    "\n",
    "$$ L_{posture} = - \\sum_{j=1}^{N} \\delta_{ij} \\log P({\\delta_{j}}|I_i)$$\n",
    "\n",
    "Combined Loss\n",
    "\n",
    "$$ L = \\lambda L_{user} + (1-\\lambda)L_{posture} $$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter that enforces the tradeoff between the two objectives - separating users and detecting postures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(lambd):\n",
    "    # Define separate loss functions for each output\n",
    "    def user_loss(y_true, y_pred):\n",
    "        return tf.keras.losses.categorical_crossentropy(y_true[0], y_pred[0])\n",
    "\n",
    "    def posture_loss(y_true, y_pred):\n",
    "        return tf.keras.losses.categorical_crossentropy(y_true[1], y_pred[1])\n",
    "\n",
    "    # Combine the losses with the specified lambda\n",
    "    def combined_loss(y_true, y_pred):\n",
    "        return lambd * user_loss(y_true, y_pred) + (1 - lambd) * posture_loss(y_true, y_pred)\n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Initial learning rate\n",
    "initial_learning_rate = 5e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oh = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "label_s_oh = tf.keras.utils.to_categorical(label_s, num_classes=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 01:08:43.116872: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inbase_cnn_model/dropout_474/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 5s 10ms/step - loss: 0.2917 - subject_output_loss: -0.4168 - posture_output_loss: 0.7086 - subject_output_categorical_accuracy: 0.0848 - posture_output_categorical_accuracy: 0.3300 - val_loss: -0.0099 - val_subject_output_loss: -0.0048 - val_posture_output_loss: -0.0050 - val_subject_output_categorical_accuracy: 0.1113 - val_posture_output_categorical_accuracy: 0.2210\n",
      "Epoch 2/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 0.2064 - subject_output_loss: 0.3168 - posture_output_loss: -0.1104 - subject_output_categorical_accuracy: 0.0857 - posture_output_categorical_accuracy: 0.4521 - val_loss: -0.0098 - val_subject_output_loss: -0.0074 - val_posture_output_loss: -0.0024 - val_subject_output_categorical_accuracy: 0.0983 - val_posture_output_categorical_accuracy: 0.5301\n",
      "Epoch 3/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: -0.9502 - subject_output_loss: -0.4313 - posture_output_loss: -0.5189 - subject_output_categorical_accuracy: 0.0932 - posture_output_categorical_accuracy: 0.5291 - val_loss: -0.0061 - val_subject_output_loss: -0.0052 - val_posture_output_loss: -8.9667e-04 - val_subject_output_categorical_accuracy: 0.1011 - val_posture_output_categorical_accuracy: 0.5301\n",
      "Epoch 4/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: -0.0529 - subject_output_loss: 0.2622 - posture_output_loss: -0.3151 - subject_output_categorical_accuracy: 0.1009 - posture_output_categorical_accuracy: 0.5282 - val_loss: -0.0072 - val_subject_output_loss: -0.0029 - val_posture_output_loss: -0.0043 - val_subject_output_categorical_accuracy: 0.1011 - val_posture_output_categorical_accuracy: 0.5301\n",
      "Epoch 5/40\n",
      "147/147 [==============================] - 2s 10ms/step - loss: 0.5000 - subject_output_loss: 0.6215 - posture_output_loss: -0.1215 - subject_output_categorical_accuracy: 0.1062 - posture_output_categorical_accuracy: 0.4778 - val_loss: -0.0110 - val_subject_output_loss: -0.0054 - val_posture_output_loss: -0.0056 - val_subject_output_categorical_accuracy: 0.1244 - val_posture_output_categorical_accuracy: 0.5363\n",
      "Epoch 6/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: -0.5461 - subject_output_loss: -0.2996 - posture_output_loss: -0.2466 - subject_output_categorical_accuracy: 0.1227 - posture_output_categorical_accuracy: 0.4379 - val_loss: -2.1729e-04 - val_subject_output_loss: -0.0025 - val_posture_output_loss: 0.0023 - val_subject_output_categorical_accuracy: 0.1244 - val_posture_output_categorical_accuracy: 0.2332\n",
      "Epoch 7/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 0.2159 - subject_output_loss: 0.1002 - posture_output_loss: 0.1158 - subject_output_categorical_accuracy: 0.0874 - posture_output_categorical_accuracy: 0.2537 - val_loss: -0.0090 - val_subject_output_loss: -0.0044 - val_posture_output_loss: -0.0047 - val_subject_output_categorical_accuracy: 0.0644 - val_posture_output_categorical_accuracy: 0.2332\n",
      "Epoch 8/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: -0.4144 - subject_output_loss: -0.6619 - posture_output_loss: 0.2475 - subject_output_categorical_accuracy: 0.0795 - posture_output_categorical_accuracy: 0.3435 - val_loss: 5.0178e-04 - val_subject_output_loss: -0.0013 - val_posture_output_loss: 0.0018 - val_subject_output_categorical_accuracy: 0.1244 - val_posture_output_categorical_accuracy: 0.2332\n",
      "Epoch 9/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: -0.3910 - subject_output_loss: -0.2948 - posture_output_loss: -0.0962 - subject_output_categorical_accuracy: 0.0695 - posture_output_categorical_accuracy: 0.2452 - val_loss: -0.0066 - val_subject_output_loss: -0.0066 - val_posture_output_loss: 0.0000e+00 - val_subject_output_categorical_accuracy: 0.0644 - val_posture_output_categorical_accuracy: 0.2332\n",
      "Epoch 10/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: -0.2802 - subject_output_loss: -0.1140 - posture_output_loss: -0.1662 - subject_output_categorical_accuracy: 0.0605 - posture_output_categorical_accuracy: 0.2339 - val_loss: 1.5280e-05 - val_subject_output_loss: 1.5280e-05 - val_posture_output_loss: 0.0000e+00 - val_subject_output_categorical_accuracy: 0.0644 - val_posture_output_categorical_accuracy: 0.2332\n",
      "Epoch 11/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: -0.0467 - subject_output_loss: -0.1826 - posture_output_loss: 0.1359 - subject_output_categorical_accuracy: 0.0597 - posture_output_categorical_accuracy: 0.2335 - val_loss: -2.0359e-06 - val_subject_output_loss: -2.0359e-06 - val_posture_output_loss: 0.0000e+00 - val_subject_output_categorical_accuracy: 0.0644 - val_posture_output_categorical_accuracy: 0.2332\n",
      "Epoch 12/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 1.0082 - subject_output_loss: 0.3946 - posture_output_loss: 0.6136 - subject_output_categorical_accuracy: 0.0646 - posture_output_categorical_accuracy: 0.2374 - val_loss: -0.0090 - val_subject_output_loss: -0.0090 - val_posture_output_loss: 0.0000e+00 - val_subject_output_categorical_accuracy: 0.0650 - val_posture_output_categorical_accuracy: 0.2332\n",
      "Epoch 13/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 0.1721 - subject_output_loss: -0.0281 - posture_output_loss: 0.2002 - subject_output_categorical_accuracy: 0.0758 - posture_output_categorical_accuracy: 0.2937 - val_loss: -0.0018 - val_subject_output_loss: -0.0012 - val_posture_output_loss: -6.7053e-04 - val_subject_output_categorical_accuracy: 0.1011 - val_posture_output_categorical_accuracy: 0.4975\n",
      "Epoch 14/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: -0.9139 - subject_output_loss: -0.1440 - posture_output_loss: -0.7699 - subject_output_categorical_accuracy: 0.0958 - posture_output_categorical_accuracy: 0.4880 - val_loss: -0.0028 - val_subject_output_loss: 0.0000e+00 - val_posture_output_loss: -0.0028 - val_subject_output_categorical_accuracy: 0.1011 - val_posture_output_categorical_accuracy: 0.4807\n",
      "Epoch 15/40\n",
      " 31/147 [=====>........................] - ETA: 0s - loss: -1.1357 - subject_output_loss: 0.0000e+00 - posture_output_loss: -1.1357 - subject_output_categorical_accuracy: 0.0958 - posture_output_categorical_accuracy: 0.4153"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39minitial_learning_rate)\n\u001b[1;32m     13\u001b[0m base_cnn_model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m optimizer,\n\u001b[1;32m     14\u001b[0m         loss\u001b[38;5;241m=\u001b[39mcustom_loss(lambd),\n\u001b[1;32m     15\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mCategoricalAccuracy()])\n\u001b[0;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m base_cnn_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     18\u001b[0m     X[train_index], \n\u001b[1;32m     19\u001b[0m     {\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_output\u001b[39m\u001b[38;5;124m'\u001b[39m: label_s_oh[train_index], \n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposture_output\u001b[39m\u001b[38;5;124m'\u001b[39m: y_oh[train_index]\n\u001b[1;32m     22\u001b[0m     },\n\u001b[1;32m     23\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs, \n\u001b[1;32m     24\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     25\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m     26\u001b[0m         X[test_index], \n\u001b[1;32m     27\u001b[0m         {\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_output\u001b[39m\u001b[38;5;124m'\u001b[39m: label_s_oh[test_index], \n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposture_output\u001b[39m\u001b[38;5;124m'\u001b[39m: y_oh[test_index]\n\u001b[1;32m     30\u001b[0m         }\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m performances[lambd]\u001b[38;5;241m.\u001b[39mappend([history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_subject_output_categorical_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m], history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_posture_output_categorical_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/envs/sof/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/sof/lib/python3.11/site-packages/keras/engine/training.py:1676\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1674\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m-> 1676\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1677\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m             epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m         ):\n\u001b[1;32m   1684\u001b[0m             callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/anaconda3/envs/sof/lib/python3.11/site-packages/keras/engine/data_adapter.py:1375\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1375\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1376\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1377\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m original_spe\n\u001b[1;32m   1380\u001b[0m )\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/anaconda3/envs/sof/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    646\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_value()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    648\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    649\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sof/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:774\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;124;03m  The value of the variable.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 774\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_variable_op()\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39midentity(value)\n",
      "File \u001b[0;32m~/anaconda3/envs/sof/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:753\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    751\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 753\u001b[0m   result \u001b[38;5;241m=\u001b[39m read_and_set_handle(no_copy)\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    756\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[1;32m    757\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[1;32m    758\u001b[0m   tape\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[1;32m    759\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[1;32m    760\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[1;32m    761\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[0;32m~/anaconda3/envs/sof/lib/python3.11/site-packages/tensorflow/python/ops/resource_variable_ops.py:743\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[0;34m(no_copy)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat\u001b[38;5;241m.\u001b[39mforward_compatible(\u001b[38;5;241m2022\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    742\u001b[0m   gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mdisable_copy_on_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m--> 743\u001b[0m result \u001b[38;5;241m=\u001b[39m gen_resource_variable_ops\u001b[38;5;241m.\u001b[39mread_variable_op(\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n\u001b[1;32m    745\u001b[0m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/sof/lib/python3.11/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py:524\u001b[0m, in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    523\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[1;32m    525\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, resource, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype)\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    527\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 64\n",
    "\n",
    "performances = {}\n",
    "for lambd in [.5]:\n",
    "    performances[lambd] = []\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {i}:\")\n",
    "        base_cnn_model=create_model()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "        base_cnn_model.compile(optimizer = optimizer,\n",
    "                loss=custom_loss(lambd),\n",
    "                metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "        history = base_cnn_model.fit(\n",
    "            X[train_index], \n",
    "            {\n",
    "                'subject_output': label_s_oh[train_index], \n",
    "                'posture_output': y_oh[train_index]\n",
    "            },\n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size, \n",
    "            validation_data=(\n",
    "                X[test_index], \n",
    "                {\n",
    "                    'subject_output': label_s_oh[test_index], \n",
    "                    'posture_output': y_oh[test_index]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        performances[lambd].append([history.history[\"val_subject_output_categorical_accuracy\"], history.history[\"val_posture_output_categorical_accuracy\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.41100690364837644, 0.65595383644104, 0.3922888934612274]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean(np.max([el[0] for el in perf], axis=1)) for perf in performances.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1869,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_s[test_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 22:50:10.200082: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inbase_cnn_model/dropout_30/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/237 [==============================] - 10s 23ms/step - loss: 5.5044 - subject_output_loss: 3.9287 - posture_output_loss: 1.5756 - subject_output_categorical_accuracy: 0.1071 - posture_output_categorical_accuracy: 0.4611 - val_loss: 3.5174 - val_subject_output_loss: 2.5361 - val_posture_output_loss: 0.9813 - val_subject_output_categorical_accuracy: 0.1907 - val_posture_output_categorical_accuracy: 0.6387\n",
      "Epoch 2/40\n",
      "237/237 [==============================] - 4s 18ms/step - loss: 3.9271 - subject_output_loss: 2.8494 - posture_output_loss: 1.0777 - subject_output_categorical_accuracy: 0.1270 - posture_output_categorical_accuracy: 0.5231 - val_loss: 3.4439 - val_subject_output_loss: 2.5277 - val_posture_output_loss: 0.9162 - val_subject_output_categorical_accuracy: 0.1468 - val_posture_output_categorical_accuracy: 0.5805\n",
      "Epoch 3/40\n",
      "237/237 [==============================] - 6s 24ms/step - loss: 3.4830 - subject_output_loss: 2.6211 - posture_output_loss: 0.8619 - subject_output_categorical_accuracy: 0.1385 - posture_output_categorical_accuracy: 0.6054 - val_loss: 3.3768 - val_subject_output_loss: 2.5535 - val_posture_output_loss: 0.8233 - val_subject_output_categorical_accuracy: 0.1729 - val_posture_output_categorical_accuracy: 0.7903\n",
      "Epoch 4/40\n",
      "237/237 [==============================] - 3s 11ms/step - loss: 3.3040 - subject_output_loss: 2.4972 - posture_output_loss: 0.8068 - subject_output_categorical_accuracy: 0.1482 - posture_output_categorical_accuracy: 0.6587 - val_loss: 3.0489 - val_subject_output_loss: 2.4606 - val_posture_output_loss: 0.5883 - val_subject_output_categorical_accuracy: 0.2038 - val_posture_output_categorical_accuracy: 0.8010\n",
      "Epoch 5/40\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 3.3038 - subject_output_loss: 2.6054 - posture_output_loss: 0.6984 - subject_output_categorical_accuracy: 0.1507 - posture_output_categorical_accuracy: 0.7292 - val_loss: 3.0520 - val_subject_output_loss: 2.4262 - val_posture_output_loss: 0.6257 - val_subject_output_categorical_accuracy: 0.2335 - val_posture_output_categorical_accuracy: 0.7754\n",
      "Epoch 6/40\n",
      "237/237 [==============================] - 4s 16ms/step - loss: 3.0578 - subject_output_loss: 2.4027 - posture_output_loss: 0.6551 - subject_output_categorical_accuracy: 0.1861 - posture_output_categorical_accuracy: 0.7530 - val_loss: 2.8892 - val_subject_output_loss: 2.4422 - val_posture_output_loss: 0.4470 - val_subject_output_categorical_accuracy: 0.2270 - val_posture_output_categorical_accuracy: 0.8723\n",
      "Epoch 7/40\n",
      "237/237 [==============================] - 4s 15ms/step - loss: 3.0962 - subject_output_loss: 2.5129 - posture_output_loss: 0.5832 - subject_output_categorical_accuracy: 0.1888 - posture_output_categorical_accuracy: 0.7618 - val_loss: 2.9957 - val_subject_output_loss: 2.5412 - val_posture_output_loss: 0.4546 - val_subject_output_categorical_accuracy: 0.1224 - val_posture_output_categorical_accuracy: 0.8728\n",
      "Epoch 8/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 3.0940 - subject_output_loss: 2.5110 - posture_output_loss: 0.5831 - subject_output_categorical_accuracy: 0.1919 - posture_output_categorical_accuracy: 0.7962 - val_loss: 2.9551 - val_subject_output_loss: 2.4653 - val_posture_output_loss: 0.4897 - val_subject_output_categorical_accuracy: 0.2282 - val_posture_output_categorical_accuracy: 0.9031\n",
      "Epoch 9/40\n",
      "237/237 [==============================] - 3s 13ms/step - loss: 2.9347 - subject_output_loss: 2.3752 - posture_output_loss: 0.5595 - subject_output_categorical_accuracy: 0.2007 - posture_output_categorical_accuracy: 0.8025 - val_loss: 2.9339 - val_subject_output_loss: 2.5317 - val_posture_output_loss: 0.4022 - val_subject_output_categorical_accuracy: 0.2068 - val_posture_output_categorical_accuracy: 0.8740\n",
      "Epoch 10/40\n",
      "237/237 [==============================] - 4s 16ms/step - loss: 2.8585 - subject_output_loss: 2.3414 - posture_output_loss: 0.5171 - subject_output_categorical_accuracy: 0.1970 - posture_output_categorical_accuracy: 0.8120 - val_loss: 2.9372 - val_subject_output_loss: 2.4720 - val_posture_output_loss: 0.4652 - val_subject_output_categorical_accuracy: 0.1480 - val_posture_output_categorical_accuracy: 0.8099\n",
      "Epoch 11/40\n",
      "237/237 [==============================] - 3s 11ms/step - loss: 2.8958 - subject_output_loss: 2.4998 - posture_output_loss: 0.3960 - subject_output_categorical_accuracy: 0.1947 - posture_output_categorical_accuracy: 0.8236 - val_loss: 2.8281 - val_subject_output_loss: 2.4720 - val_posture_output_loss: 0.3561 - val_subject_output_categorical_accuracy: 0.2305 - val_posture_output_categorical_accuracy: 0.8913\n",
      "Epoch 12/40\n",
      "237/237 [==============================] - 3s 11ms/step - loss: 2.8177 - subject_output_loss: 2.4140 - posture_output_loss: 0.4038 - subject_output_categorical_accuracy: 0.2101 - posture_output_categorical_accuracy: 0.8366 - val_loss: 2.6683 - val_subject_output_loss: 2.4045 - val_posture_output_loss: 0.2638 - val_subject_output_categorical_accuracy: 0.2436 - val_posture_output_categorical_accuracy: 0.9216\n",
      "Epoch 13/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.5356 - subject_output_loss: 2.2310 - posture_output_loss: 0.3046 - subject_output_categorical_accuracy: 0.2158 - posture_output_categorical_accuracy: 0.8408 - val_loss: 3.3541 - val_subject_output_loss: 2.6931 - val_posture_output_loss: 0.6610 - val_subject_output_categorical_accuracy: 0.1075 - val_posture_output_categorical_accuracy: 0.7213\n",
      "Epoch 14/40\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 2.7743 - subject_output_loss: 2.4047 - posture_output_loss: 0.3695 - subject_output_categorical_accuracy: 0.2110 - posture_output_categorical_accuracy: 0.8491 - val_loss: 2.7944 - val_subject_output_loss: 2.4586 - val_posture_output_loss: 0.3358 - val_subject_output_categorical_accuracy: 0.1800 - val_posture_output_categorical_accuracy: 0.9156\n",
      "Epoch 15/40\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2.5517 - subject_output_loss: 2.1731 - posture_output_loss: 0.3786 - subject_output_categorical_accuracy: 0.2219 - posture_output_categorical_accuracy: 0.8578 - val_loss: 2.5734 - val_subject_output_loss: 2.3869 - val_posture_output_loss: 0.1864 - val_subject_output_categorical_accuracy: 0.2793 - val_posture_output_categorical_accuracy: 0.9727\n",
      "Epoch 16/40\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 2.6548 - subject_output_loss: 2.2725 - posture_output_loss: 0.3823 - subject_output_categorical_accuracy: 0.2234 - posture_output_categorical_accuracy: 0.8556 - val_loss: 2.5923 - val_subject_output_loss: 2.3895 - val_posture_output_loss: 0.2027 - val_subject_output_categorical_accuracy: 0.2733 - val_posture_output_categorical_accuracy: 0.9020\n",
      "Epoch 17/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.7450 - subject_output_loss: 2.3974 - posture_output_loss: 0.3475 - subject_output_categorical_accuracy: 0.2218 - posture_output_categorical_accuracy: 0.8786 - val_loss: 2.5575 - val_subject_output_loss: 2.3480 - val_posture_output_loss: 0.2095 - val_subject_output_categorical_accuracy: 0.2329 - val_posture_output_categorical_accuracy: 0.9584\n",
      "Epoch 18/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.5479 - subject_output_loss: 2.2723 - posture_output_loss: 0.2757 - subject_output_categorical_accuracy: 0.2397 - posture_output_categorical_accuracy: 0.8944 - val_loss: 2.5741 - val_subject_output_loss: 2.3084 - val_posture_output_loss: 0.2657 - val_subject_output_categorical_accuracy: 0.2050 - val_posture_output_categorical_accuracy: 0.9542\n",
      "Epoch 19/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.5094 - subject_output_loss: 2.1198 - posture_output_loss: 0.3896 - subject_output_categorical_accuracy: 0.2483 - posture_output_categorical_accuracy: 0.8841 - val_loss: 2.7633 - val_subject_output_loss: 2.4539 - val_posture_output_loss: 0.3094 - val_subject_output_categorical_accuracy: 0.3387 - val_posture_output_categorical_accuracy: 0.9376\n",
      "Epoch 20/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.5132 - subject_output_loss: 2.1758 - posture_output_loss: 0.3375 - subject_output_categorical_accuracy: 0.2411 - posture_output_categorical_accuracy: 0.8951 - val_loss: 2.3844 - val_subject_output_loss: 2.2592 - val_posture_output_loss: 0.1252 - val_subject_output_categorical_accuracy: 0.3393 - val_posture_output_categorical_accuracy: 0.9899\n",
      "Epoch 21/40\n",
      "237/237 [==============================] - 2s 8ms/step - loss: 2.5429 - subject_output_loss: 2.2310 - posture_output_loss: 0.3119 - subject_output_categorical_accuracy: 0.2473 - posture_output_categorical_accuracy: 0.8903 - val_loss: 2.4920 - val_subject_output_loss: 2.2445 - val_posture_output_loss: 0.2474 - val_subject_output_categorical_accuracy: 0.3559 - val_posture_output_categorical_accuracy: 0.9554\n",
      "Epoch 22/40\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2.4829 - subject_output_loss: 2.1947 - posture_output_loss: 0.2882 - subject_output_categorical_accuracy: 0.2572 - posture_output_categorical_accuracy: 0.9001 - val_loss: 2.5494 - val_subject_output_loss: 2.3294 - val_posture_output_loss: 0.2200 - val_subject_output_categorical_accuracy: 0.3642 - val_posture_output_categorical_accuracy: 0.9887\n",
      "Epoch 23/40\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2.3459 - subject_output_loss: 2.1026 - posture_output_loss: 0.2433 - subject_output_categorical_accuracy: 0.2656 - posture_output_categorical_accuracy: 0.9186 - val_loss: 2.4480 - val_subject_output_loss: 2.3313 - val_posture_output_loss: 0.1167 - val_subject_output_categorical_accuracy: 0.3512 - val_posture_output_categorical_accuracy: 0.9792\n",
      "Epoch 24/40\n",
      "237/237 [==============================] - 3s 11ms/step - loss: 2.3745 - subject_output_loss: 2.1056 - posture_output_loss: 0.2689 - subject_output_categorical_accuracy: 0.2568 - posture_output_categorical_accuracy: 0.9161 - val_loss: 2.4286 - val_subject_output_loss: 2.2555 - val_posture_output_loss: 0.1731 - val_subject_output_categorical_accuracy: 0.3749 - val_posture_output_categorical_accuracy: 0.9935\n",
      "Epoch 25/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.4323 - subject_output_loss: 2.1121 - posture_output_loss: 0.3202 - subject_output_categorical_accuracy: 0.2612 - posture_output_categorical_accuracy: 0.9151 - val_loss: 2.3238 - val_subject_output_loss: 2.2057 - val_posture_output_loss: 0.1181 - val_subject_output_categorical_accuracy: 0.3755 - val_posture_output_categorical_accuracy: 0.9947\n",
      "Epoch 26/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.4652 - subject_output_loss: 2.2176 - posture_output_loss: 0.2476 - subject_output_categorical_accuracy: 0.2574 - posture_output_categorical_accuracy: 0.9214 - val_loss: 2.3369 - val_subject_output_loss: 2.1560 - val_posture_output_loss: 0.1809 - val_subject_output_categorical_accuracy: 0.2478 - val_posture_output_categorical_accuracy: 0.9899\n",
      "Epoch 27/40\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 2.5030 - subject_output_loss: 2.2208 - posture_output_loss: 0.2822 - subject_output_categorical_accuracy: 0.2592 - posture_output_categorical_accuracy: 0.9109 - val_loss: 2.2243 - val_subject_output_loss: 2.1433 - val_posture_output_loss: 0.0811 - val_subject_output_categorical_accuracy: 0.3304 - val_posture_output_categorical_accuracy: 0.9887\n",
      "Epoch 28/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.4367 - subject_output_loss: 2.1697 - posture_output_loss: 0.2669 - subject_output_categorical_accuracy: 0.2703 - posture_output_categorical_accuracy: 0.9058 - val_loss: 2.4781 - val_subject_output_loss: 2.3498 - val_posture_output_loss: 0.1283 - val_subject_output_categorical_accuracy: 0.1687 - val_posture_output_categorical_accuracy: 0.9893\n",
      "Epoch 29/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.3245 - subject_output_loss: 2.1353 - posture_output_loss: 0.1892 - subject_output_categorical_accuracy: 0.2827 - posture_output_categorical_accuracy: 0.9247 - val_loss: 2.3819 - val_subject_output_loss: 2.2150 - val_posture_output_loss: 0.1670 - val_subject_output_categorical_accuracy: 0.2496 - val_posture_output_categorical_accuracy: 0.9798\n",
      "Epoch 30/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.2222 - subject_output_loss: 2.0546 - posture_output_loss: 0.1676 - subject_output_categorical_accuracy: 0.2765 - posture_output_categorical_accuracy: 0.9305 - val_loss: 2.2895 - val_subject_output_loss: 2.1807 - val_posture_output_loss: 0.1088 - val_subject_output_categorical_accuracy: 0.2757 - val_posture_output_categorical_accuracy: 0.9929\n",
      "Epoch 31/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.2195 - subject_output_loss: 2.0166 - posture_output_loss: 0.2029 - subject_output_categorical_accuracy: 0.2669 - posture_output_categorical_accuracy: 0.9295 - val_loss: 2.1318 - val_subject_output_loss: 2.0838 - val_posture_output_loss: 0.0480 - val_subject_output_categorical_accuracy: 0.3928 - val_posture_output_categorical_accuracy: 0.9952\n",
      "Epoch 32/40\n",
      "237/237 [==============================] - 1s 6ms/step - loss: 2.2826 - subject_output_loss: 1.9992 - posture_output_loss: 0.2834 - subject_output_categorical_accuracy: 0.2936 - posture_output_categorical_accuracy: 0.9354 - val_loss: 2.5425 - val_subject_output_loss: 2.4107 - val_posture_output_loss: 0.1317 - val_subject_output_categorical_accuracy: 0.2876 - val_posture_output_categorical_accuracy: 0.9881\n",
      "Epoch 33/40\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2.1639 - subject_output_loss: 1.9804 - posture_output_loss: 0.1834 - subject_output_categorical_accuracy: 0.2983 - posture_output_categorical_accuracy: 0.9308 - val_loss: 2.0672 - val_subject_output_loss: 2.0243 - val_posture_output_loss: 0.0430 - val_subject_output_categorical_accuracy: 0.4462 - val_posture_output_categorical_accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2.3378 - subject_output_loss: 2.0594 - posture_output_loss: 0.2784 - subject_output_categorical_accuracy: 0.2870 - posture_output_categorical_accuracy: 0.9171 - val_loss: 2.5720 - val_subject_output_loss: 2.3059 - val_posture_output_loss: 0.2661 - val_subject_output_categorical_accuracy: 0.1919 - val_posture_output_categorical_accuracy: 0.9150\n",
      "Epoch 35/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.3394 - subject_output_loss: 2.0538 - posture_output_loss: 0.2857 - subject_output_categorical_accuracy: 0.2919 - posture_output_categorical_accuracy: 0.9300 - val_loss: 2.3220 - val_subject_output_loss: 2.1168 - val_posture_output_loss: 0.2052 - val_subject_output_categorical_accuracy: 0.3405 - val_posture_output_categorical_accuracy: 0.9168\n",
      "Epoch 36/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.2476 - subject_output_loss: 2.0324 - posture_output_loss: 0.2151 - subject_output_categorical_accuracy: 0.2903 - posture_output_categorical_accuracy: 0.9272 - val_loss: 2.0519 - val_subject_output_loss: 1.9907 - val_posture_output_loss: 0.0612 - val_subject_output_categorical_accuracy: 0.3595 - val_posture_output_categorical_accuracy: 0.9970\n",
      "Epoch 37/40\n",
      "237/237 [==============================] - 2s 10ms/step - loss: 2.1584 - subject_output_loss: 1.9798 - posture_output_loss: 0.1787 - subject_output_categorical_accuracy: 0.2935 - posture_output_categorical_accuracy: 0.9466 - val_loss: 2.0969 - val_subject_output_loss: 2.0632 - val_posture_output_loss: 0.0336 - val_subject_output_categorical_accuracy: 0.4106 - val_posture_output_categorical_accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "237/237 [==============================] - 2s 9ms/step - loss: 2.2628 - subject_output_loss: 2.0796 - posture_output_loss: 0.1831 - subject_output_categorical_accuracy: 0.2979 - posture_output_categorical_accuracy: 0.9400 - val_loss: 2.2115 - val_subject_output_loss: 2.1442 - val_posture_output_loss: 0.0672 - val_subject_output_categorical_accuracy: 0.2929 - val_posture_output_categorical_accuracy: 0.9958\n",
      "Epoch 39/40\n",
      "237/237 [==============================] - 2s 7ms/step - loss: 2.1822 - subject_output_loss: 2.0298 - posture_output_loss: 0.1524 - subject_output_categorical_accuracy: 0.3010 - posture_output_categorical_accuracy: 0.9311 - val_loss: 2.1756 - val_subject_output_loss: 2.1195 - val_posture_output_loss: 0.0561 - val_subject_output_categorical_accuracy: 0.3975 - val_posture_output_categorical_accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "237/237 [==============================] - 2s 6ms/step - loss: 2.2625 - subject_output_loss: 2.0501 - posture_output_loss: 0.2124 - subject_output_categorical_accuracy: 0.2853 - posture_output_categorical_accuracy: 0.9431 - val_loss: 1.9989 - val_subject_output_loss: 1.9613 - val_posture_output_loss: 0.0375 - val_subject_output_categorical_accuracy: 0.4046 - val_posture_output_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Defining the loss functions and training\n",
    "\n",
    "# Loss function: multi-class cross-entropy loss of users and their postures\n",
    "# Use a combined loss function that is a weighted sum of both individual losses\n",
    "\n",
    "# Train with different values of lambda from 0 to 1 \n",
    "\n",
    "batch_size =  64\n",
    "epochs = 40\n",
    "\n",
    "#The network is trained for 40 epochs, using Adam optimizer with learningrate of 10−3, decayed of 0.95 every 10 epochs\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "\n",
    "# Validation Schemes: k-fold and LOSO (Leave one subject out only only posture classification)\n",
    "lambd = 0.05 # value between 0 and 1\n",
    "\n",
    "base_cnn_model.compile(optimizer = optimizer,\n",
    "              loss=custom_loss(lambd),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "history = base_cnn_model.fit(X_train, {'subject_output': label_s_train, 'posture_output': y_train},\n",
    "                    epochs=epochs, batch_size=batch_size, validation_split=0.1, shuffle = True)\n",
    "\n",
    "end = time.time()\n",
    "#For our final model, we chose the best λ for each validation scheme (λ = 0.5 for k-Fold cross validation and λ = 0.2 for LOSO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CNN utils\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitioning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
