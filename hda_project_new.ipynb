{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bed Posture and Subject Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set which GPU to use\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.layers import Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import ndimage\n",
    "\n",
    "# Other\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used data from the first experiment in the public PmatData dataset.\n",
    "\n",
    "Information about the dataset:\n",
    "- Samples are collected using Vista Medical FSA Soft- Flex 2048\n",
    "- The main data folder contains 13 directories corresponding to 13 subjects. Each folder contains 17 txt files, which correspond to the 17 possible sleeping postures (8 standard and 9 uncommon postures)\n",
    "- Each file contains multiple recordings of the subject posture in the form of pressure matrices, sampled at 1Hz. Each row is length 2,048 representing the 64x32 pressure mat size. Pressure ratings range from 0 to 1000\n",
    "- 2-mins (around 120 frames) of data was recorded per subject\n",
    "\n",
    "Source of dataset: https://physionet.org/content/pmd/1.0.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Height/cm</th>\n",
       "      <th>Weight/kg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subject-Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>183</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>183</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>177</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>172</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>169</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>179</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>186</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>174</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>176</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33</td>\n",
       "      <td>170</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age  Height/cm  Weight/kg\n",
       "Subject-Number                           \n",
       "1                19        175         87\n",
       "2                23        183         85\n",
       "3                23        183        100\n",
       "4                24        177         70\n",
       "5                24        172         66\n",
       "6                26        169         83\n",
       "7                27        179         96\n",
       "8                27        186         63\n",
       "9                30        174         74\n",
       "10               30        174         79\n",
       "11               30        176         91\n",
       "12               33        170         78\n",
       "13               34        174         74"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subj_inf = pd.read_csv(path + '/data/experiment-i/subject-info-i.csv', encoding = 'utf_16', index_col='Subject-Number')\n",
    "display(subj_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subjects in Experiment I are: ['S9', 'S10', 'S12', 'S7', 'S6', 'S4', 'S1', 'S8', 'S11', 'S3', 'S13', 'S2', 'S5']\n"
     ]
    }
   ],
   "source": [
    "# 13 subjects in Experiment I\n",
    "subj_list = [file for file in os.listdir(path +'/data/experiment-i/') if os.path.isdir(os.path.join(path +'/data/experiment-i/', file))]\n",
    "print(f\"The subjects in Experiment I are: {subj_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positions for each subject are: 17\n"
     ]
    }
   ],
   "source": [
    "positions_cnt = [file for file in os.listdir(path +'/data/experiment-i/S5/') if os.path.isfile(os.path.join(path +'/data/experiment-i/S5/', file))]\n",
    "print(f\"The number of positions for each subject are: {len(positions_cnt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position vector\n",
    "positions_i = [\"placeholder\", \"supine\", \"right\",\n",
    "                     \"left\", \"right\", \"right\",\n",
    "                     \"left\", \"left\", \"supine\",\n",
    "                     \"supine\", \"supine\", \"supine\",\n",
    "                     \"supine\", \"right\", \"left\",\n",
    "                     \"supine\", \"supine\", \"supine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use position names to define 3 common positions\n",
    "\n",
    "def token_position(x):\n",
    "    return {\n",
    "        'supine': 0,\n",
    "        'left': 1,\n",
    "        'right': 2,\n",
    "        'left_fetus': 1,\n",
    "        'right_fetus': 2\n",
    "    }[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 2048)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect one file\n",
    "test_read = np.loadtxt(path + '/data/experiment-i/S5/1.txt')\n",
    "test_read.shape # (101, 2048) = (number of samples, number of sensors (32x64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Test read - Experiment I, Subject 5, Position 1\")\n",
    "fig = plt.imshow(test_read[20,:].reshape(64, 32)) # 20th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first two measurements in each file are corrupted\n",
    "fig, axes = plt.subplots(figsize=(10,6), ncols=3)\n",
    "fig.suptitle(\"Corrupted images - Experiment I\")\n",
    "for ii in range(3):\n",
    "    ax = axes[ii]\n",
    "    ax.imshow(test_read[ii,:].reshape(64,32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['S9', 'S10', 'S12', 'S7', 'S6', 'S4', 'S1', 'S8', 'S11', 'S3', 'S13', 'S2', 'S5'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all the data into a dictionary, where the keys are the subjects and \n",
    "# values: images (index 0) and their position's number (index 1)\n",
    "\n",
    "exp_i_data = {}\n",
    "\n",
    "for _, dirs, _ in os.walk(path + '/data/experiment-i/'): # root, directories, files: indices for os.walk    \n",
    "    for dir in dirs:\n",
    "        subject = dir\n",
    "        data = None\n",
    "        labels = None\n",
    "        for _, _, files in os.walk(path + '/data/experiment-i/' + dir):\n",
    "            for file in files:\n",
    "                file_path = path + '/data/experiment-i/' + dir + '/' + file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    # remove first and last 3 frames of each sequence\n",
    "                    for line in f.read().splitlines()[3:-3]:\n",
    "                        raw_data = np.fromstring(line, dtype=float, sep='\\t').reshape(1, 64, 32)\n",
    "                        # apply median filter of 3x3\n",
    "                        raw_data = ndimage.median_filter(raw_data, size=(1,3,3))\n",
    "                        # Normalize the data: change range from [0-1000] to [0-255]\n",
    "                        file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
    "                        # Label the data using position number from file name\n",
    "                        file_label = token_position(positions_i[int(file[:-4])])                        \n",
    "                        file_label = np.array([file_label])\n",
    "\n",
    "                        if data is None:\n",
    "                            data = file_data\n",
    "                        else:\n",
    "                            # Concatenate the new data along the first axis\n",
    "                            data = np.concatenate((data, file_data), axis=0)\n",
    "                        if labels is None:\n",
    "                            labels = file_label\n",
    "                        else:\n",
    "                            labels = np.concatenate((labels, file_label), axis=0)\n",
    "                        \n",
    "        exp_i_data[subject] = {'data': data, 'labels': labels}\n",
    "\n",
    "exp_i_data.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the labels\n",
    "def token_patient(x):\n",
    "    return {'S1': 0, 'S2': 1, 'S3': 2, 'S4': 3, 'S5': 4,  'S6': 5, 'S7': 6,\n",
    "                    'S8': 7, 'S9': 8, 'S10': 9, 'S11': 10, 'S12': 11, 'S13':12}[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each experiment\n",
    "\n",
    "# Experiment I\n",
    "X = exp_i_data[subj_list[0]]['data']\n",
    "y = exp_i_data[subj_list[0]]['labels']\n",
    "label_s = np.full(len(X), token_patient(subj_list[0]))\n",
    "\n",
    "for subject in subj_list[1:]:\n",
    "    X = np.append(X, exp_i_data[subject]['data'], axis=0)\n",
    "    y = np.append(y, exp_i_data[subject]['labels'], axis=0)\n",
    "    label_s = np.append(label_s, np.full(len(exp_i_data[subject]['data']), token_patient(subject)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment I: X shape: (18698, 64, 32), y shape: (18698,)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes of the datasets\n",
    "print(f\"Experiment I: X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "\n",
    "# 10% was used for testing and 90% for training.\n",
    "\n",
    "# Experiment I\n",
    "# indices = np.arange(len(X))\n",
    "# X_train, X_test, y_train, y_test, indices_train, indices_test, = train_test_split(X, y, indices, test_size=0.1, random_state=123)\n",
    "\n",
    "# # Split label_s accordingly using indices\n",
    "# label_s_train = label_s[indices_train]\n",
    "# label_s_test = label_s[indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_s_train = tf.keras.utils.to_categorical(label_s_train, num_classes=13)\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment I: Training data shape: (16828, 64, 32), Training labels shape: (16828, 3)\n",
      "Experiment I: Testing data shape: (1870, 64, 32), Testing labels shape: (1870,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the training and testing datasets\n",
    "\n",
    "# print(f\"Experiment I: Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
    "# print(f\"Experiment I: Testing data shape: {X_test.shape}, Testing labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proposed model from paper\n",
    "\n",
    "# can't use a sequential model as model has multiple outputs\n",
    "#each convolutional block was followed by an increasing dropout rate of 10%, 20%, 30%, 40%. \n",
    "\n",
    "# Define the functional model\n",
    "def create_model(regularization=0):\n",
    "\n",
    "    input = tf.keras.Input(shape=(64, 32, 1), name=\"img\")\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), strides=(1,1), padding='valid', kernel_regularizer=l2(regularization))(input)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.MaxPool2D((3,3), strides=(2, 2), padding='valid')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalMaxPool2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    subject_output = tf.keras.layers.Dense(13, activation='softmax', name=\"subject_output\")(x)\n",
    "    posture_output = tf.keras.layers.Dense(3, activation='softmax', name=\"posture_output\")(x)\n",
    "\n",
    "    base_cnn_model = tf.keras.Model(inputs=input, outputs=[subject_output,posture_output], name=\"base_cnn_model\")\n",
    "\n",
    "    return base_cnn_model\n",
    "    # From Figure 2 in paper\n",
    "\n",
    "    # Input: 32x64x1\n",
    "\n",
    "    # 4 main blocks of conv-batchnorm plus max pool for 1st 2 blocks\n",
    "    # 1st block: 30 x 62 x 32 of conv-batchnorm-maxpool-leakyrelu (32 filters applied), max pool would make each 30x 62 =10 x 20, cov would make (32-3+1)x(64-3+1) = 30 x 62\n",
    "    # 2nd block: 13 x 29 x 64 of conv-batchnorm-maxpool-leakyrelu\n",
    "    # 3rd block: 4 x 12 x 128 of conv-batchnorm-leakyrelu\n",
    "    # 4th block: 2 x 10 x 256 of conv-batchnorm-leakyrelu\n",
    "\n",
    "    # Two Dense layers of 256 units with dropout of 0.5\n",
    "    # Two output layers, multinomial logistic regressors for recognition of subjects (M units) and their sleep postures (N units)\n",
    "\n",
    "    # Loss function: multiclass cross-entropy loss function of subjects and users separately\n",
    "    # plus combined loss function of both with hyperparameter lambda (from 0 to 1)\n",
    "\n",
    "    # Conv kernels: 3x3, stride 1, padding valid\n",
    "    # Max pool: 3x3\n",
    "    # batch norm\n",
    "    # Leaky ReLU: alpha 0.2\n",
    "\n",
    "    # dense layers have dropout of 0.5\n",
    "\n",
    "    # each conv block was followed by increasing drop of .1, .2, .3, .4\n",
    "\n",
    "    # L2 reg loss with coefficient of 0.002\n",
    "    # 2 softmax regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# median filter of 3x3\n",
    "# remove first and last 3 frames of each sequence\n",
    "# threshold filtering and equalization histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions for Training\n",
    "\n",
    "### Notation\n",
    "- Let $I$ be the input pressure map.\n",
    "- Let $i$ be the example index.\n",
    "- Let $\\gamma$ and $\\delta$ be the user and the posture, respectively\n",
    "- Let M and N be the number of users and postures, respectively\n",
    "\n",
    "### Multi-Class Cross Entropy Loss Functions\n",
    "\n",
    "User Loss:\n",
    "\n",
    "$$ L_{user} = - \\sum_{j=1}^{M} \\gamma_{ij} \\log P({\\gamma_{j}}|I_i)$$\n",
    "\n",
    "\n",
    "Posture Loss:\n",
    "\n",
    "$$ L_{posture} = - \\sum_{j=1}^{N} \\delta_{ij} \\log P({\\delta_{j}}|I_i)$$\n",
    "\n",
    "Combined Loss\n",
    "\n",
    "$$ L = \\lambda L_{user} + (1-\\lambda)L_{posture} $$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter that enforces the tradeoff between the two objectives - separating users and detecting postures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(lambd):\n",
    "    # Define separate loss functions for each output\n",
    "    def user_loss(y_true, y_pred):\n",
    "        return tf.keras.losses.categorical_crossentropy(y_true[0], y_pred[0])\n",
    "\n",
    "    def posture_loss(y_true, y_pred):\n",
    "        return tf.keras.losses.categorical_crossentropy(y_true[1], y_pred[1])\n",
    "\n",
    "    # Combine the losses with the specified lambda\n",
    "    def combined_loss(y_true, y_pred):\n",
    "        return lambd * user_loss(y_true, y_pred) + (1 - lambd) * posture_loss(y_true, y_pred)\n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Initial learning rate\n",
    "initial_learning_rate = 5e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oh = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "label_s_oh = tf.keras.utils.to_categorical(label_s, num_classes=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 64\n",
    "regularization = 0\n",
    "lambdas = [0.2, 0.5, 0.6, 0.8]\n",
    "\n",
    "performances = {}\n",
    "for lambd in lambdas:\n",
    "    performances[lambd] = []\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {i}:\")\n",
    "        base_cnn_model=create_model(regularization)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "        base_cnn_model.compile(optimizer = optimizer,\n",
    "                loss=custom_loss(lambd),\n",
    "                metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "        history = base_cnn_model.fit(\n",
    "            X[train_index], \n",
    "            {\n",
    "                'subject_output': label_s_oh[train_index], \n",
    "                'posture_output': y_oh[train_index]\n",
    "            },\n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size, \n",
    "            validation_data=(\n",
    "                X[test_index], \n",
    "                {\n",
    "                    'subject_output': label_s_oh[test_index], \n",
    "                    'posture_output': y_oh[test_index]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        performances[lambd].append([history.history[\"val_subject_output_categorical_accuracy\"], history.history[\"val_posture_output_categorical_accuracy\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean performance for each lambda on subject accuracy\n",
      "[0.5157762408256531, 0.5996379733085633, 0.6129009485244751, 0.5156704246997833]\n",
      "----------------------------------------------------\n",
      "Mean performance for each lambda on posture accuracy\n",
      "[0.9995721817016602, 0.9995721817016602, 0.9995186686515808, 0.9996791124343872]\n"
     ]
    }
   ],
   "source": [
    "# mean performances for each lamda on subject [0] or posture [1] accuracy\n",
    "\n",
    "print('Mean performance for each lambda on subject accuracy')\n",
    "print([np.mean(np.max([el[0] for el in perf], axis=1)) for perf in performances.values()])\n",
    "print('----------------------------------------------------')\n",
    "print('Mean performance for each lambda on posture accuracy')\n",
    "print([np.mean(np.max([el[1] for el in perf], axis=1)) for perf in performances.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
