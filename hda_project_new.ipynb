{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bed Posture and Subject Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 11:03:45.698369: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-28 11:03:46.429677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Data Load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.layers import Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import ndimage\n",
    "#import skimage.transform\n",
    "\n",
    "# Other\n",
    "import math\n",
    "import h5py\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Height/cm</th>\n",
       "      <th>Weight/kg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subject-Number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>183</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>183</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>177</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>172</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>169</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>179</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>186</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>174</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30</td>\n",
       "      <td>176</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>33</td>\n",
       "      <td>170</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age  Height/cm  Weight/kg\n",
       "Subject-Number                           \n",
       "1                19        175         87\n",
       "2                23        183         85\n",
       "3                23        183        100\n",
       "4                24        177         70\n",
       "5                24        172         66\n",
       "6                26        169         83\n",
       "7                27        179         96\n",
       "8                27        186         63\n",
       "9                30        174         74\n",
       "10               30        174         79\n",
       "11               30        176         91\n",
       "12               33        170         78\n",
       "13               34        174         74"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "subj_inf = pd.read_csv(path + '/data/experiment-i/subject-info-i.csv', encoding = 'utf_16', index_col='Subject-Number')\n",
    "display(subj_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subjects in Experiment I are: ['S9', 'S10', 'S12', 'S7', 'S6', 'S4', 'S1', 'S8', 'S11', 'S3', 'S13', 'S2', 'S5']\n"
     ]
    }
   ],
   "source": [
    "# 13 subjects in Experiment I\n",
    "subj_list = [file for file in os.listdir(path +'/data/experiment-i/') if os.path.isdir(os.path.join(path +'/data/experiment-i/', file))]\n",
    "print(f\"The subjects in Experiment I are: {subj_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positions for each subject are: 17\n"
     ]
    }
   ],
   "source": [
    "positions_cnt = [file for file in os.listdir(path +'/data/experiment-i/S5/') if os.path.isfile(os.path.join(path +'/data/experiment-i/S5/', file))]\n",
    "print(f\"The number of positions for each subject are: {len(positions_cnt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position vector\n",
    "positions_i = [\"placeholder\", \"supine\", \"right\",\n",
    "                     \"left\", \"right\", \"right\",\n",
    "                     \"left\", \"left\", \"supine\",\n",
    "                     \"supine\", \"supine\", \"supine\",\n",
    "                     \"supine\", \"right\", \"left\",\n",
    "                     \"supine\", \"supine\", \"supine\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use position names to define 3 common positions\n",
    "\n",
    "def token_position(x):\n",
    "    return {\n",
    "        'supine': 0,\n",
    "        'left': 1,\n",
    "        'right': 2,\n",
    "        'left_fetus': 1,\n",
    "        'right_fetus': 2\n",
    "    }[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 2048)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect one file\n",
    "test_read = np.loadtxt(path + '/data/experiment-i/S5/1.txt')\n",
    "test_read.shape # (101, 2048) = (number of samples, number of sensors (32x64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Test read - Experiment I, Subject 5, Position 1\")\n",
    "fig = plt.imshow(test_read[20,:].reshape(64, 32)) # 20th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first two measurements in each file are corrupted\n",
    "fig, axes = plt.subplots(figsize=(10,6), ncols=3)\n",
    "fig.suptitle(\"Corrupted images - Experiment I\")\n",
    "for ii in range(3):\n",
    "    ax = axes[ii]\n",
    "    ax.imshow(test_read[ii,:].reshape(64,32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['S9', 'S10', 'S12', 'S7', 'S6', 'S4', 'S1', 'S8', 'S11', 'S3', 'S13', 'S2', 'S5'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all the data into a dictionary, where the keys are the subjects and \n",
    "# values: images (index 0) and their position's number (index 1)\n",
    "\n",
    "exp_i_data = {}\n",
    "\n",
    "for _, dirs, _ in os.walk(path + '/data/experiment-i/'): # root, directories, files: indices for os.walk    \n",
    "    for dir in dirs:\n",
    "        subject = dir\n",
    "        data = None\n",
    "        labels = None\n",
    "        for _, _, files in os.walk(path + '/data/experiment-i/' + dir):\n",
    "            for file in files:\n",
    "                file_path = path + '/data/experiment-i/' + dir + '/' + file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    # remove first and last 3 frames of each sequence\n",
    "                    for line in f.read().splitlines()[3:-3]:\n",
    "                        raw_data = np.fromstring(line, dtype=float, sep='\\t').reshape(1, 64, 32)\n",
    "                        # apply median filter of 3x3\n",
    "                        raw_data = ndimage.median_filter(raw_data, size=(1,3,3))\n",
    "                        # Normalize the data: change range from [0-1000] to [0-255]\n",
    "                        file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
    "                        # Label the data using position number from file name\n",
    "                        file_label = token_position(positions_i[int(file[:-4])])                        \n",
    "                        file_label = np.array([file_label])\n",
    "\n",
    "                        if data is None:\n",
    "                            data = file_data\n",
    "                        else:\n",
    "                            # Concatenate the new data along the first axis\n",
    "                            data = np.concatenate((data, file_data), axis=0)\n",
    "                        if labels is None:\n",
    "                            labels = file_label\n",
    "                        else:\n",
    "                            labels = np.concatenate((labels, file_label), axis=0)\n",
    "                        \n",
    "        exp_i_data[subject] = {'data': data, 'labels': labels}\n",
    "\n",
    "exp_i_data.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the labels\n",
    "def token_patient(x):\n",
    "    return {'S1': 0, 'S2': 1, 'S3': 2, 'S4': 3, 'S5': 4,  'S6': 5, 'S7': 6,\n",
    "                    'S8': 7, 'S9': 8, 'S10': 9, 'S11': 10, 'S12': 11, 'S13':12}[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each experiment\n",
    "\n",
    "# Experiment I\n",
    "X = exp_i_data[subj_list[0]]['data']\n",
    "y = exp_i_data[subj_list[0]]['labels']\n",
    "label_s = np.full(len(X), token_patient(subj_list[0]))\n",
    "\n",
    "for subject in subj_list[1:]:\n",
    "    X = np.append(X, exp_i_data[subject]['data'], axis=0)\n",
    "    y = np.append(y, exp_i_data[subject]['labels'], axis=0)\n",
    "    label_s = np.append(label_s, np.full(len(exp_i_data[subject]['data']), token_patient(subject)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment I: X shape: (18698, 64, 32), y shape: (18698,)\n"
     ]
    }
   ],
   "source": [
    "# print the shapes of the datasets\n",
    "print(f\"Experiment I: X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "\n",
    "# 10% was used for testing and 90% for training.\n",
    "\n",
    "# Experiment I\n",
    "# indices = np.arange(len(X))\n",
    "# X_train, X_test, y_train, y_test, indices_train, indices_test, = train_test_split(X, y, indices, test_size=0.1, random_state=123)\n",
    "\n",
    "# # Split label_s accordingly using indices\n",
    "# label_s_train = label_s[indices_train]\n",
    "# label_s_test = label_s[indices_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_s_train = tf.keras.utils.to_categorical(label_s_train, num_classes=13)\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment I: Training data shape: (16828, 64, 32), Training labels shape: (16828, 3)\n",
      "Experiment I: Testing data shape: (1870, 64, 32), Testing labels shape: (1870,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the training and testing datasets\n",
    "\n",
    "# print(f\"Experiment I: Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
    "# print(f\"Experiment I: Testing data shape: {X_test.shape}, Testing labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the proposed model from paper\n",
    "\n",
    "# can't use a sequential model as model has multiple outputs\n",
    "#each convolutional block was followed by an increasing dropout rate of 10%, 20%, 30%, 40%. \n",
    "\n",
    "# Define the functional model\n",
    "def create_model(regularization=0):\n",
    "\n",
    "    input = tf.keras.Input(shape=(64, 32, 1), name=\"img\")\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), strides=(1,1), padding='valid', kernel_regularizer=l2(regularization))(input)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.MaxPool2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.MaxPool2D((3,3), strides=(2, 2), padding='valid')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(256, (3, 3), strides=(1,1), padding='valid', activation=None, kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalMaxPool2D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=l2(regularization))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    subject_output = tf.keras.layers.Dense(13, activation='softmax', name=\"subject_output\")(x)\n",
    "    posture_output = tf.keras.layers.Dense(3, activation='softmax', name=\"posture_output\")(x)\n",
    "\n",
    "    base_cnn_model = tf.keras.Model(inputs=input, outputs=[subject_output,posture_output], name=\"base_cnn_model\")\n",
    "\n",
    "    return base_cnn_model\n",
    "    # From Figure 2 in paper\n",
    "\n",
    "    # Input: 32x64x1\n",
    "\n",
    "    # 4 main blocks of conv-batchnorm plus max pool for 1st 2 blocks\n",
    "    # 1st block: 30 x 62 x 32 of conv-batchnorm-maxpool-leakyrelu (32 filters applied), max pool would make each 30x 62 =10 x 20, cov would make (32-3+1)x(64-3+1) = 30 x 62\n",
    "    # 2nd block: 13 x 29 x 64 of conv-batchnorm-maxpool-leakyrelu\n",
    "    # 3rd block: 4 x 12 x 128 of conv-batchnorm-leakyrelu\n",
    "    # 4th block: 2 x 10 x 256 of conv-batchnorm-leakyrelu\n",
    "\n",
    "    # Two Dense layers of 256 units with dropout of 0.5\n",
    "    # Two output layers, multinomial logistic regressors for recognition of subjects (M units) and their sleep postures (N units)\n",
    "\n",
    "    # Loss function: multiclass cross-entropy loss function of subjects and users separately\n",
    "    # plus combined loss function of both with hyperparameter lambda (from 0 to 1)\n",
    "\n",
    "    # Conv kernels: 3x3, stride 1, padding valid\n",
    "    # Max pool: 3x3\n",
    "    # batch norm\n",
    "    # Leaky ReLU: alpha 0.2\n",
    "\n",
    "    # dense layers have dropout of 0.5\n",
    "\n",
    "    # each conv block was followed by increasing drop of .1, .2, .3, .4\n",
    "\n",
    "    # L2 reg loss with coefficient of 0.002\n",
    "    # 2 softmax regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# median filter of 3x3\n",
    "# remove first and last 3 frames of each sequence\n",
    "# threshold filtering and equalization histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions for Training\n",
    "\n",
    "### Notation\n",
    "- Let $I$ be the input pressure map.\n",
    "- Let $i$ be the example index.\n",
    "- Let $\\gamma$ and $\\delta$ be the user and the posture, respectively\n",
    "- Let M and N be the number of users and postures, respectively\n",
    "\n",
    "### Multi-Class Cross Entropy Loss Functions\n",
    "\n",
    "User Loss:\n",
    "\n",
    "$$ L_{user} = - \\sum_{j=1}^{M} \\gamma_{ij} \\log P({\\gamma_{j}}|I_i)$$\n",
    "\n",
    "\n",
    "Posture Loss:\n",
    "\n",
    "$$ L_{posture} = - \\sum_{j=1}^{N} \\delta_{ij} \\log P({\\delta_{j}}|I_i)$$\n",
    "\n",
    "Combined Loss\n",
    "\n",
    "$$ L = \\lambda L_{user} + (1-\\lambda)L_{posture} $$\n",
    "\n",
    "where $\\lambda$ is a hyperparameter that enforces the tradeoff between the two objectives - separating users and detecting postures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(lambd):\n",
    "    # Define separate loss functions for each output\n",
    "    def user_loss(y_true, y_pred):\n",
    "        return tf.keras.losses.categorical_crossentropy(y_true[0], y_pred[0])\n",
    "\n",
    "    def posture_loss(y_true, y_pred):\n",
    "        return tf.keras.losses.categorical_crossentropy(y_true[1], y_pred[1])\n",
    "\n",
    "    # Combine the losses with the specified lambda\n",
    "    def combined_loss(y_true, y_pred):\n",
    "        return lambd * user_loss(y_true, y_pred) + (1 - lambd) * posture_loss(y_true, y_pred)\n",
    "    return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Initial learning rate\n",
    "initial_learning_rate = 5e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_oh = tf.keras.utils.to_categorical(y, num_classes=3)\n",
    "label_s_oh = tf.keras.utils.to_categorical(label_s, num_classes=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 11:07:07.542921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:07.610718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:07.611029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:07.617713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:07.617978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:07.618222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:08.150341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentat"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ion/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:08.150612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:08.150843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-28 11:07:08.151040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21552 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-06-28 11:07:09.712468: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inbase_cnn_model/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-06-28 11:07:09.884391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8907\n",
      "2024-06-28 11:07:10.804766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-06-28 11:07:10.808731: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x76190c3c5490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-28 11:07:10.808743: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-06-28 11:07:10.812195: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-28 11:07:10.895480: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 6s 13ms/step - loss: 5.5384 - subject_output_loss: 3.8337 - posture_output_loss: 1.7048 - subject_output_categorical_accuracy: 0.1125 - posture_output_categorical_accuracy: 0.4687 - val_loss: 3.6087 - val_subject_output_loss: 2.6808 - val_posture_output_loss: 0.9279 - val_subject_output_categorical_accuracy: 0.1684 - val_posture_output_categorical_accuracy: 0.6209\n",
      "Epoch 2/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 4.0124 - subject_output_loss: 2.8920 - posture_output_loss: 1.1204 - subject_output_categorical_accuracy: 0.1174 - posture_output_categorical_accuracy: 0.5368 - val_loss: 3.3215 - val_subject_output_loss: 2.4620 - val_posture_output_loss: 0.8595 - val_subject_output_categorical_accuracy: 0.1769 - val_posture_output_categorical_accuracy: 0.7379\n",
      "Epoch 3/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 3.5184 - subject_output_loss: 2.6048 - posture_output_loss: 0.9136 - subject_output_categorical_accuracy: 0.1260 - posture_output_categorical_accuracy: 0.5946 - val_loss: 3.2881 - val_subject_output_loss: 2.5101 - val_posture_output_loss: 0.7781 - val_subject_output_categorical_accuracy: 0.1342 - val_posture_output_categorical_accuracy: 0.6814\n",
      "Epoch 4/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 3.4033 - subject_output_loss: 2.6386 - posture_output_loss: 0.7646 - subject_output_categorical_accuracy: 0.1446 - posture_output_categorical_accuracy: 0.6545 - val_loss: 3.2450 - val_subject_output_loss: 2.4806 - val_posture_output_loss: 0.7644 - val_subject_output_categorical_accuracy: 0.2519 - val_posture_output_categorical_accuracy: 0.7969\n",
      "Epoch 5/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 3.4077 - subject_output_loss: 2.6268 - posture_output_loss: 0.7808 - subject_output_categorical_accuracy: 0.1553 - posture_output_categorical_accuracy: 0.6922 - val_loss: 3.1406 - val_subject_output_loss: 2.4360 - val_posture_output_loss: 0.7046 - val_subject_output_categorical_accuracy: 0.2352 - val_posture_output_categorical_accuracy: 0.7680\n",
      "Epoch 6/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 3.1992 - subject_output_loss: 2.5192 - posture_output_loss: 0.6800 - subject_output_categorical_accuracy: 0.1736 - posture_output_categorical_accuracy: 0.7368 - val_loss: 3.1394 - val_subject_output_loss: 2.4409 - val_posture_output_loss: 0.6986 - val_subject_output_categorical_accuracy: 0.2039 - val_posture_output_categorical_accuracy: 0.8504\n",
      "Epoch 7/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 3.0982 - subject_output_loss: 2.5682 - posture_output_loss: 0.5300 - subject_output_categorical_accuracy: 0.1757 - posture_output_categorical_accuracy: 0.7655 - val_loss: 2.8551 - val_subject_output_loss: 2.3584 - val_posture_output_loss: 0.4967 - val_subject_output_categorical_accuracy: 0.2372 - val_posture_output_categorical_accuracy: 0.8912\n",
      "Epoch 8/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 3.0140 - subject_output_loss: 2.4423 - posture_output_loss: 0.5717 - subject_output_categorical_accuracy: 0.1784 - posture_output_categorical_accuracy: 0.7740 - val_loss: 2.9400 - val_subject_output_loss: 2.4089 - val_posture_output_loss: 0.5311 - val_subject_output_categorical_accuracy: 0.1817 - val_posture_output_categorical_accuracy: 0.8312\n",
      "Epoch 9/40\n",
      "147/147 [==============================] - 2s 10ms/step - loss: 2.7721 - subject_output_loss: 2.2868 - posture_output_loss: 0.4853 - subject_output_categorical_accuracy: 0.2024 - posture_output_categorical_accuracy: 0.8050 - val_loss: 2.7432 - val_subject_output_loss: 2.3417 - val_posture_output_loss: 0.4015 - val_subject_output_categorical_accuracy: 0.2195 - val_posture_output_categorical_accuracy: 0.9616\n",
      "Epoch 10/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.8884 - subject_output_loss: 2.4006 - posture_output_loss: 0.4877 - subject_output_categorical_accuracy: 0.1994 - posture_output_categorical_accuracy: 0.8106 - val_loss: 2.7358 - val_subject_output_loss: 2.2830 - val_posture_output_loss: 0.4528 - val_subject_output_categorical_accuracy: 0.2729 - val_posture_output_categorical_accuracy: 0.8592\n",
      "Epoch 11/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.7973 - subject_output_loss: 2.3724 - posture_output_loss: 0.4249 - subject_output_categorical_accuracy: 0.2037 - posture_output_categorical_accuracy: 0.8354 - val_loss: 2.6401 - val_subject_output_loss: 2.3012 - val_posture_output_loss: 0.3389 - val_subject_output_categorical_accuracy: 0.2149 - val_posture_output_categorical_accuracy: 0.9604\n",
      "Epoch 12/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.7766 - subject_output_loss: 2.3796 - posture_output_loss: 0.3970 - subject_output_categorical_accuracy: 0.2037 - posture_output_categorical_accuracy: 0.8500 - val_loss: 2.5211 - val_subject_output_loss: 2.2060 - val_posture_output_loss: 0.3151 - val_subject_output_categorical_accuracy: 0.2644 - val_posture_output_categorical_accuracy: 0.9080\n",
      "Epoch 13/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.7168 - subject_output_loss: 2.3613 - posture_output_loss: 0.3556 - subject_output_categorical_accuracy: 0.2279 - posture_output_categorical_accuracy: 0.8530 - val_loss: 2.5775 - val_subject_output_loss: 2.1917 - val_posture_output_loss: 0.3858 - val_subject_output_categorical_accuracy: 0.2593 - val_posture_output_categorical_accuracy: 0.9353\n",
      "Epoch 14/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.6313 - subject_output_loss: 2.2386 - posture_output_loss: 0.3927 - subject_output_categorical_accuracy: 0.2258 - posture_output_categorical_accuracy: 0.8575 - val_loss: 2.5019 - val_subject_output_loss: 2.2080 - val_posture_output_loss: 0.2939 - val_subject_output_categorical_accuracy: 0.2287 - val_posture_output_categorical_accuracy: 0.9330\n",
      "Epoch 15/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.5652 - subject_output_loss: 2.2588 - posture_output_loss: 0.3064 - subject_output_categorical_accuracy: 0.2276 - posture_output_categorical_accuracy: 0.8689 - val_loss: 2.5045 - val_subject_output_loss: 2.2419 - val_posture_output_loss: 0.2626 - val_subject_output_categorical_accuracy: 0.2721 - val_posture_output_categorical_accuracy: 0.9546\n",
      "Epoch 16/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.5117 - subject_output_loss: 2.1939 - posture_output_loss: 0.3178 - subject_output_categorical_accuracy: 0.2377 - posture_output_categorical_accuracy: 0.8684 - val_loss: 2.3711 - val_subject_output_loss: 2.1224 - val_posture_output_loss: 0.2487 - val_subject_output_categorical_accuracy: 0.2805 - val_posture_output_categorical_accuracy: 0.9866\n",
      "Epoch 17/40\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 2.6260 - subject_output_loss: 2.3535 - posture_output_loss: 0.2725 - subject_output_categorical_accuracy: 0.2409 - posture_output_categorical_accuracy: 0.8833 - val_loss: 2.3320 - val_subject_output_loss: 2.0837 - val_posture_output_loss: 0.2483 - val_subject_output_categorical_accuracy: 0.3641 - val_posture_output_categorical_accuracy: 0.9737\n",
      "Epoch 18/40\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 2.4656 - subject_output_loss: 2.1228 - posture_output_loss: 0.3427 - subject_output_categorical_accuracy: 0.2436 - posture_output_categorical_accuracy: 0.9022 - val_loss: 2.3702 - val_subject_output_loss: 2.1787 - val_posture_output_loss: 0.1915 - val_subject_output_categorical_accuracy: 0.2114 - val_posture_output_categorical_accuracy: 0.9978\n",
      "Epoch 19/40\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 2.6343 - subject_output_loss: 2.3290 - posture_output_loss: 0.3054 - subject_output_categorical_accuracy: 0.2463 - posture_output_categorical_accuracy: 0.8959 - val_loss: 2.2946 - val_subject_output_loss: 2.1043 - val_posture_output_loss: 0.1903 - val_subject_output_categorical_accuracy: 0.3667 - val_posture_output_categorical_accuracy: 0.9824\n",
      "Epoch 20/40\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 2.3565 - subject_output_loss: 2.0921 - posture_output_loss: 0.2644 - subject_output_categorical_accuracy: 0.2535 - posture_output_categorical_accuracy: 0.8846 - val_loss: 2.2119 - val_subject_output_loss: 2.0050 - val_posture_output_loss: 0.2069 - val_subject_output_categorical_accuracy: 0.3715 - val_posture_output_categorical_accuracy: 0.9773\n",
      "Epoch 21/40\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 2.4710 - subject_output_loss: 2.1759 - posture_output_loss: 0.2951 - subject_output_categorical_accuracy: 0.2522 - posture_output_categorical_accuracy: 0.8973 - val_loss: 2.6593 - val_subject_output_loss: 2.2881 - val_posture_output_loss: 0.3712 - val_subject_output_categorical_accuracy: 0.2100 - val_posture_output_categorical_accuracy: 0.8762\n",
      "Epoch 22/40\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 2.4153 - subject_output_loss: 2.1947 - posture_output_loss: 0.2206 - subject_output_categorical_accuracy: 0.2563 - posture_output_categorical_accuracy: 0.9148 - val_loss: 2.1909 - val_subject_output_loss: 2.0340 - val_posture_output_loss: 0.1569 - val_subject_output_categorical_accuracy: 0.3747 - val_posture_output_categorical_accuracy: 0.9987\n",
      "Epoch 23/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.5955 - subject_output_loss: 2.3059 - posture_output_loss: 0.2896 - subject_output_categorical_accuracy: 0.2505 - posture_output_categorical_accuracy: 0.9070 - val_loss: 2.2053 - val_subject_output_loss: 1.9918 - val_posture_output_loss: 0.2135 - val_subject_output_categorical_accuracy: 0.3254 - val_posture_output_categorical_accuracy: 0.9841\n",
      "Epoch 24/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.3822 - subject_output_loss: 2.1698 - posture_output_loss: 0.2125 - subject_output_categorical_accuracy: 0.2633 - posture_output_categorical_accuracy: 0.9115 - val_loss: 2.9907 - val_subject_output_loss: 2.6532 - val_posture_output_loss: 0.3376 - val_subject_output_categorical_accuracy: 0.1358 - val_posture_output_categorical_accuracy: 0.8881\n",
      "Epoch 25/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.5057 - subject_output_loss: 2.1970 - posture_output_loss: 0.3087 - subject_output_categorical_accuracy: 0.2735 - posture_output_categorical_accuracy: 0.9015 - val_loss: 2.1039 - val_subject_output_loss: 1.9714 - val_posture_output_loss: 0.1324 - val_subject_output_categorical_accuracy: 0.3858 - val_posture_output_categorical_accuracy: 0.9940\n",
      "Epoch 26/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.3164 - subject_output_loss: 2.0688 - posture_output_loss: 0.2476 - subject_output_categorical_accuracy: 0.2638 - posture_output_categorical_accuracy: 0.9131 - val_loss: 2.1211 - val_subject_output_loss: 1.9642 - val_posture_output_loss: 0.1569 - val_subject_output_categorical_accuracy: 0.3907 - val_posture_output_categorical_accuracy: 0.9799\n",
      "Epoch 27/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.2885 - subject_output_loss: 2.1321 - posture_output_loss: 0.1563 - subject_output_categorical_accuracy: 0.2733 - posture_output_categorical_accuracy: 0.9191 - val_loss: 2.0748 - val_subject_output_loss: 1.9471 - val_posture_output_loss: 0.1277 - val_subject_output_categorical_accuracy: 0.3941 - val_posture_output_categorical_accuracy: 0.9889\n",
      "Epoch 28/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.3564 - subject_output_loss: 2.0334 - posture_output_loss: 0.3230 - subject_output_categorical_accuracy: 0.2829 - posture_output_categorical_accuracy: 0.9024 - val_loss: 2.7264 - val_subject_output_loss: 2.4591 - val_posture_output_loss: 0.2673 - val_subject_output_categorical_accuracy: 0.1363 - val_posture_output_categorical_accuracy: 0.9529\n",
      "Epoch 29/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.2481 - subject_output_loss: 2.0724 - posture_output_loss: 0.1757 - subject_output_categorical_accuracy: 0.2820 - posture_output_categorical_accuracy: 0.9195 - val_loss: 2.2907 - val_subject_output_loss: 2.1334 - val_posture_output_loss: 0.1573 - val_subject_output_categorical_accuracy: 0.2504 - val_posture_output_categorical_accuracy: 0.9947\n",
      "Epoch 30/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.3152 - subject_output_loss: 2.0227 - posture_output_loss: 0.2926 - subject_output_categorical_accuracy: 0.2848 - posture_output_categorical_accuracy: 0.9123 - val_loss: 2.0796 - val_subject_output_loss: 1.9404 - val_posture_output_loss: 0.1392 - val_subject_output_categorical_accuracy: 0.3504 - val_posture_output_categorical_accuracy: 0.9990\n",
      "Epoch 31/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.2261 - subject_output_loss: 1.9634 - posture_output_loss: 0.2627 - subject_output_categorical_accuracy: 0.2781 - posture_output_categorical_accuracy: 0.9312 - val_loss: 2.1042 - val_subject_output_loss: 1.9446 - val_posture_output_loss: 0.1596 - val_subject_output_categorical_accuracy: 0.3440 - val_posture_output_categorical_accuracy: 0.9527\n",
      "Epoch 32/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.3394 - subject_output_loss: 2.1168 - posture_output_loss: 0.2226 - subject_output_categorical_accuracy: 0.2758 - posture_output_categorical_accuracy: 0.9137 - val_loss: 1.9768 - val_subject_output_loss: 1.8663 - val_posture_output_loss: 0.1105 - val_subject_output_categorical_accuracy: 0.3934 - val_posture_output_categorical_accuracy: 0.9984\n",
      "Epoch 33/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.3011 - subject_output_loss: 2.0618 - posture_output_loss: 0.2393 - subject_output_categorical_accuracy: 0.2877 - posture_output_categorical_accuracy: 0.9210 - val_loss: 2.1353 - val_subject_output_loss: 2.0439 - val_posture_output_loss: 0.0913 - val_subject_output_categorical_accuracy: 0.3106 - val_posture_output_categorical_accuracy: 0.9975\n",
      "Epoch 34/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.2466 - subject_output_loss: 2.0223 - posture_output_loss: 0.2242 - subject_output_categorical_accuracy: 0.2900 - posture_output_categorical_accuracy: 0.9339 - val_loss: 2.0120 - val_subject_output_loss: 1.8757 - val_posture_output_loss: 0.1363 - val_subject_output_categorical_accuracy: 0.3958 - val_posture_output_categorical_accuracy: 0.9983\n",
      "Epoch 35/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.3085 - subject_output_loss: 2.1094 - posture_output_loss: 0.1991 - subject_output_categorical_accuracy: 0.2812 - posture_output_categorical_accuracy: 0.9241 - val_loss: 2.0332 - val_subject_output_loss: 1.8687 - val_posture_output_loss: 0.1646 - val_subject_output_categorical_accuracy: 0.4231 - val_posture_output_categorical_accuracy: 0.9732\n",
      "Epoch 36/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.1601 - subject_output_loss: 1.9892 - posture_output_loss: 0.1709 - subject_output_categorical_accuracy: 0.2906 - posture_output_categorical_accuracy: 0.9295 - val_loss: 1.9735 - val_subject_output_loss: 1.8715 - val_posture_output_loss: 0.1020 - val_subject_output_categorical_accuracy: 0.3889 - val_posture_output_categorical_accuracy: 0.9979\n",
      "Epoch 37/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.2142 - subject_output_loss: 2.0198 - posture_output_loss: 0.1944 - subject_output_categorical_accuracy: 0.3030 - posture_output_categorical_accuracy: 0.9317 - val_loss: 1.9172 - val_subject_output_loss: 1.8230 - val_posture_output_loss: 0.0942 - val_subject_output_categorical_accuracy: 0.4097 - val_posture_output_categorical_accuracy: 0.9933\n",
      "Epoch 38/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.1505 - subject_output_loss: 1.9347 - posture_output_loss: 0.2157 - subject_output_categorical_accuracy: 0.3140 - posture_output_categorical_accuracy: 0.9291 - val_loss: 1.9350 - val_subject_output_loss: 1.8404 - val_posture_output_loss: 0.0945 - val_subject_output_categorical_accuracy: 0.3536 - val_posture_output_categorical_accuracy: 0.9966\n",
      "Epoch 39/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.2007 - subject_output_loss: 2.0081 - posture_output_loss: 0.1926 - subject_output_categorical_accuracy: 0.3068 - posture_output_categorical_accuracy: 0.9256 - val_loss: 2.0289 - val_subject_output_loss: 1.8896 - val_posture_output_loss: 0.1393 - val_subject_output_categorical_accuracy: 0.4135 - val_posture_output_categorical_accuracy: 0.9955\n",
      "Epoch 40/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.0990 - subject_output_loss: 1.9364 - posture_output_loss: 0.1627 - subject_output_categorical_accuracy: 0.3121 - posture_output_categorical_accuracy: 0.9217 - val_loss: 1.9849 - val_subject_output_loss: 1.9222 - val_posture_output_loss: 0.0627 - val_subject_output_categorical_accuracy: 0.2993 - val_posture_output_categorical_accuracy: 0.9990\n",
      "Fold 1:\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 11:08:05.501793: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inbase_cnn_model/dropout_6/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 4s 11ms/step - loss: 5.8210 - subject_output_loss: 3.9484 - posture_output_loss: 1.8726 - subject_output_categorical_accuracy: 0.1109 - posture_output_categorical_accuracy: 0.4450 - val_loss: 3.5754 - val_subject_output_loss: 2.5488 - val_posture_output_loss: 1.0266 - val_subject_output_categorical_accuracy: 0.1389 - val_posture_output_categorical_accuracy: 0.6343\n",
      "Epoch 2/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 3.9410 - subject_output_loss: 2.8745 - posture_output_loss: 1.0665 - subject_output_categorical_accuracy: 0.1165 - posture_output_categorical_accuracy: 0.5488 - val_loss: 3.3832 - val_subject_output_loss: 2.4696 - val_posture_output_loss: 0.9137 - val_subject_output_categorical_accuracy: 0.1524 - val_posture_output_categorical_accuracy: 0.7199\n",
      "Epoch 3/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 3.5222 - subject_output_loss: 2.6491 - posture_output_loss: 0.8731 - subject_output_categorical_accuracy: 0.1183 - posture_output_categorical_accuracy: 0.6297 - val_loss: 3.1886 - val_subject_output_loss: 2.4570 - val_posture_output_loss: 0.7317 - val_subject_output_categorical_accuracy: 0.2133 - val_posture_output_categorical_accuracy: 0.8653\n",
      "Epoch 4/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 3.3730 - subject_output_loss: 2.6306 - posture_output_loss: 0.7424 - subject_output_categorical_accuracy: 0.1282 - posture_output_categorical_accuracy: 0.7045 - val_loss: 3.0610 - val_subject_output_loss: 2.4220 - val_posture_output_loss: 0.6390 - val_subject_output_categorical_accuracy: 0.1995 - val_posture_output_categorical_accuracy: 0.8077\n",
      "Epoch 5/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 3.3527 - subject_output_loss: 2.6588 - posture_output_loss: 0.6939 - subject_output_categorical_accuracy: 0.1325 - posture_output_categorical_accuracy: 0.6962 - val_loss: 3.1081 - val_subject_output_loss: 2.4530 - val_posture_output_loss: 0.6552 - val_subject_output_categorical_accuracy: 0.2012 - val_posture_output_categorical_accuracy: 0.7809\n",
      "Epoch 6/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 3.2203 - subject_output_loss: 2.6185 - posture_output_loss: 0.6018 - subject_output_categorical_accuracy: 0.1443 - posture_output_categorical_accuracy: 0.7382 - val_loss: 3.0180 - val_subject_output_loss: 2.4756 - val_posture_output_loss: 0.5424 - val_subject_output_categorical_accuracy: 0.1746 - val_posture_output_categorical_accuracy: 0.8143\n",
      "Epoch 7/40\n",
      "147/147 [==============================] - 2s 11ms/step - loss: 3.1387 - subject_output_loss: 2.5434 - posture_output_loss: 0.5953 - subject_output_categorical_accuracy: 0.1537 - posture_output_categorical_accuracy: 0.7864 - val_loss: 2.8824 - val_subject_output_loss: 2.4069 - val_posture_output_loss: 0.4755 - val_subject_output_categorical_accuracy: 0.2216 - val_posture_output_categorical_accuracy: 0.8386\n",
      "Epoch 8/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 3.0059 - subject_output_loss: 2.5416 - posture_output_loss: 0.4644 - subject_output_categorical_accuracy: 0.1418 - posture_output_categorical_accuracy: 0.8163 - val_loss: 2.8519 - val_subject_output_loss: 2.4412 - val_posture_output_loss: 0.4107 - val_subject_output_categorical_accuracy: 0.1486 - val_posture_output_categorical_accuracy: 0.8537\n",
      "Epoch 9/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 3.1190 - subject_output_loss: 2.6631 - posture_output_loss: 0.4560 - subject_output_categorical_accuracy: 0.1312 - posture_output_categorical_accuracy: 0.8230 - val_loss: 2.8458 - val_subject_output_loss: 2.4102 - val_posture_output_loss: 0.4356 - val_subject_output_categorical_accuracy: 0.2079 - val_posture_output_categorical_accuracy: 0.8443\n",
      "Epoch 10/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.9581 - subject_output_loss: 2.5001 - posture_output_loss: 0.4581 - subject_output_categorical_accuracy: 0.1695 - posture_output_categorical_accuracy: 0.8278 - val_loss: 2.8032 - val_subject_output_loss: 2.3327 - val_posture_output_loss: 0.4704 - val_subject_output_categorical_accuracy: 0.2410 - val_posture_output_categorical_accuracy: 0.7856\n",
      "Epoch 11/40\n",
      "147/147 [==============================] - 2s 10ms/step - loss: 2.8470 - subject_output_loss: 2.4165 - posture_output_loss: 0.4305 - subject_output_categorical_accuracy: 0.1719 - posture_output_categorical_accuracy: 0.8138 - val_loss: 2.6469 - val_subject_output_loss: 2.3423 - val_posture_output_loss: 0.3046 - val_subject_output_categorical_accuracy: 0.2163 - val_posture_output_categorical_accuracy: 0.8769\n",
      "Epoch 12/40\n",
      "147/147 [==============================] - 2s 10ms/step - loss: 2.8774 - subject_output_loss: 2.5043 - posture_output_loss: 0.3731 - subject_output_categorical_accuracy: 0.1834 - posture_output_categorical_accuracy: 0.8293 - val_loss: 2.6935 - val_subject_output_loss: 2.3132 - val_posture_output_loss: 0.3803 - val_subject_output_categorical_accuracy: 0.2999 - val_posture_output_categorical_accuracy: 0.8903\n",
      "Epoch 13/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.8714 - subject_output_loss: 2.3482 - posture_output_loss: 0.5232 - subject_output_categorical_accuracy: 0.1896 - posture_output_categorical_accuracy: 0.8099 - val_loss: 2.7945 - val_subject_output_loss: 2.4325 - val_posture_output_loss: 0.3620 - val_subject_output_categorical_accuracy: 0.1740 - val_posture_output_categorical_accuracy: 0.9525\n",
      "Epoch 14/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.8396 - subject_output_loss: 2.4473 - posture_output_loss: 0.3923 - subject_output_categorical_accuracy: 0.2044 - posture_output_categorical_accuracy: 0.8285 - val_loss: 2.6216 - val_subject_output_loss: 2.3378 - val_posture_output_loss: 0.2838 - val_subject_output_categorical_accuracy: 0.2028 - val_posture_output_categorical_accuracy: 0.9070\n",
      "Epoch 15/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.7358 - subject_output_loss: 2.3526 - posture_output_loss: 0.3832 - subject_output_categorical_accuracy: 0.2064 - posture_output_categorical_accuracy: 0.8525 - val_loss: 2.4105 - val_subject_output_loss: 2.1047 - val_posture_output_loss: 0.3057 - val_subject_output_categorical_accuracy: 0.3198 - val_posture_output_categorical_accuracy: 0.9140\n",
      "Epoch 16/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.6456 - subject_output_loss: 2.2948 - posture_output_loss: 0.3508 - subject_output_categorical_accuracy: 0.2175 - posture_output_categorical_accuracy: 0.8709 - val_loss: 2.3979 - val_subject_output_loss: 2.1387 - val_posture_output_loss: 0.2592 - val_subject_output_categorical_accuracy: 0.2687 - val_posture_output_categorical_accuracy: 0.9776\n",
      "Epoch 17/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.4702 - subject_output_loss: 2.2253 - posture_output_loss: 0.2450 - subject_output_categorical_accuracy: 0.2318 - posture_output_categorical_accuracy: 0.8735 - val_loss: 2.2989 - val_subject_output_loss: 2.0736 - val_posture_output_loss: 0.2253 - val_subject_output_categorical_accuracy: 0.3323 - val_posture_output_categorical_accuracy: 0.9588\n",
      "Epoch 18/40\n",
      "147/147 [==============================] - 2s 10ms/step - loss: 2.7806 - subject_output_loss: 2.4589 - posture_output_loss: 0.3217 - subject_output_categorical_accuracy: 0.2259 - posture_output_categorical_accuracy: 0.8824 - val_loss: 2.8320 - val_subject_output_loss: 2.2807 - val_posture_output_loss: 0.5513 - val_subject_output_categorical_accuracy: 0.2322 - val_posture_output_categorical_accuracy: 0.7433\n",
      "Epoch 19/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.6183 - subject_output_loss: 2.2341 - posture_output_loss: 0.3842 - subject_output_categorical_accuracy: 0.2287 - posture_output_categorical_accuracy: 0.8383 - val_loss: 2.5976 - val_subject_output_loss: 2.3369 - val_posture_output_loss: 0.2608 - val_subject_output_categorical_accuracy: 0.2116 - val_posture_output_categorical_accuracy: 0.9802\n",
      "Epoch 20/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.6041 - subject_output_loss: 2.2950 - posture_output_loss: 0.3091 - subject_output_categorical_accuracy: 0.2310 - posture_output_categorical_accuracy: 0.8837 - val_loss: 2.4150 - val_subject_output_loss: 2.1690 - val_posture_output_loss: 0.2459 - val_subject_output_categorical_accuracy: 0.2687 - val_posture_output_categorical_accuracy: 0.9748\n",
      "Epoch 21/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.5000 - subject_output_loss: 2.2423 - posture_output_loss: 0.2578 - subject_output_categorical_accuracy: 0.2322 - posture_output_categorical_accuracy: 0.8949 - val_loss: 2.2055 - val_subject_output_loss: 1.9859 - val_posture_output_loss: 0.2196 - val_subject_output_categorical_accuracy: 0.2981 - val_posture_output_categorical_accuracy: 0.9557\n",
      "Epoch 22/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.4628 - subject_output_loss: 2.2183 - posture_output_loss: 0.2445 - subject_output_categorical_accuracy: 0.2366 - posture_output_categorical_accuracy: 0.9054 - val_loss: 2.2086 - val_subject_output_loss: 2.0169 - val_posture_output_loss: 0.1917 - val_subject_output_categorical_accuracy: 0.2776 - val_posture_output_categorical_accuracy: 0.9459\n",
      "Epoch 23/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.5482 - subject_output_loss: 2.2832 - posture_output_loss: 0.2650 - subject_output_categorical_accuracy: 0.2275 - posture_output_categorical_accuracy: 0.9042 - val_loss: 2.1712 - val_subject_output_loss: 1.9910 - val_posture_output_loss: 0.1802 - val_subject_output_categorical_accuracy: 0.3382 - val_posture_output_categorical_accuracy: 0.9865\n",
      "Epoch 24/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.4785 - subject_output_loss: 2.1963 - posture_output_loss: 0.2821 - subject_output_categorical_accuracy: 0.2484 - posture_output_categorical_accuracy: 0.9013 - val_loss: 2.1962 - val_subject_output_loss: 1.9902 - val_posture_output_loss: 0.2060 - val_subject_output_categorical_accuracy: 0.3412 - val_posture_output_categorical_accuracy: 0.9955\n",
      "Epoch 25/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.4800 - subject_output_loss: 2.2416 - posture_output_loss: 0.2384 - subject_output_categorical_accuracy: 0.2406 - posture_output_categorical_accuracy: 0.9067 - val_loss: 2.2291 - val_subject_output_loss: 2.0296 - val_posture_output_loss: 0.1996 - val_subject_output_categorical_accuracy: 0.3330 - val_posture_output_categorical_accuracy: 0.9585\n",
      "Epoch 26/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.2885 - subject_output_loss: 2.0498 - posture_output_loss: 0.2387 - subject_output_categorical_accuracy: 0.2493 - posture_output_categorical_accuracy: 0.9142 - val_loss: 2.0752 - val_subject_output_loss: 1.9901 - val_posture_output_loss: 0.0851 - val_subject_output_categorical_accuracy: 0.2870 - val_posture_output_categorical_accuracy: 0.9984\n",
      "Epoch 27/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.5333 - subject_output_loss: 2.3169 - posture_output_loss: 0.2165 - subject_output_categorical_accuracy: 0.2459 - posture_output_categorical_accuracy: 0.8999 - val_loss: 2.1577 - val_subject_output_loss: 1.9934 - val_posture_output_loss: 0.1643 - val_subject_output_categorical_accuracy: 0.3497 - val_posture_output_categorical_accuracy: 0.9922\n",
      "Epoch 28/40\n",
      "147/147 [==============================] - 2s 10ms/step - loss: 2.4388 - subject_output_loss: 2.2110 - posture_output_loss: 0.2278 - subject_output_categorical_accuracy: 0.2436 - posture_output_categorical_accuracy: 0.9303 - val_loss: 2.0086 - val_subject_output_loss: 1.8941 - val_posture_output_loss: 0.1145 - val_subject_output_categorical_accuracy: 0.3623 - val_posture_output_categorical_accuracy: 0.9993\n",
      "Epoch 29/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.3882 - subject_output_loss: 2.1791 - posture_output_loss: 0.2090 - subject_output_categorical_accuracy: 0.2648 - posture_output_categorical_accuracy: 0.9139 - val_loss: 2.0622 - val_subject_output_loss: 1.9195 - val_posture_output_loss: 0.1427 - val_subject_output_categorical_accuracy: 0.3483 - val_posture_output_categorical_accuracy: 0.9704\n",
      "Epoch 30/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.4803 - subject_output_loss: 2.2119 - posture_output_loss: 0.2684 - subject_output_categorical_accuracy: 0.2521 - posture_output_categorical_accuracy: 0.9172 - val_loss: 2.2087 - val_subject_output_loss: 2.0775 - val_posture_output_loss: 0.1312 - val_subject_output_categorical_accuracy: 0.3457 - val_posture_output_categorical_accuracy: 0.9886\n",
      "Epoch 31/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.3082 - subject_output_loss: 2.0974 - posture_output_loss: 0.2108 - subject_output_categorical_accuracy: 0.2681 - posture_output_categorical_accuracy: 0.9265 - val_loss: 2.3389 - val_subject_output_loss: 2.2376 - val_posture_output_loss: 0.1013 - val_subject_output_categorical_accuracy: 0.2091 - val_posture_output_categorical_accuracy: 0.9955\n",
      "Epoch 32/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.4447 - subject_output_loss: 2.1676 - posture_output_loss: 0.2771 - subject_output_categorical_accuracy: 0.2760 - posture_output_categorical_accuracy: 0.9203 - val_loss: 2.0260 - val_subject_output_loss: 1.8636 - val_posture_output_loss: 0.1624 - val_subject_output_categorical_accuracy: 0.3783 - val_posture_output_categorical_accuracy: 0.9860\n",
      "Epoch 33/40\n",
      "147/147 [==============================] - 1s 10ms/step - loss: 2.3871 - subject_output_loss: 2.1491 - posture_output_loss: 0.2380 - subject_output_categorical_accuracy: 0.2784 - posture_output_categorical_accuracy: 0.9062 - val_loss: 2.2031 - val_subject_output_loss: 2.0406 - val_posture_output_loss: 0.1625 - val_subject_output_categorical_accuracy: 0.2986 - val_posture_output_categorical_accuracy: 0.9782\n",
      "Epoch 34/40\n",
      "147/147 [==============================] - 1s 9ms/step - loss: 2.4987 - subject_output_loss: 2.1134 - posture_output_loss: 0.3853 - subject_output_categorical_accuracy: 0.2861 - posture_output_categorical_accuracy: 0.8999 - val_loss: 2.0841 - val_subject_output_loss: 1.9591 - val_posture_output_loss: 0.1250 - val_subject_output_categorical_accuracy: 0.3428 - val_posture_output_categorical_accuracy: 0.9994\n",
      "Epoch 35/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.1851 - subject_output_loss: 1.9869 - posture_output_loss: 0.1982 - subject_output_categorical_accuracy: 0.2868 - posture_output_categorical_accuracy: 0.9193 - val_loss: 2.0308 - val_subject_output_loss: 1.9402 - val_posture_output_loss: 0.0906 - val_subject_output_categorical_accuracy: 0.3492 - val_posture_output_categorical_accuracy: 0.9919\n",
      "Epoch 36/40\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 2.3317 - subject_output_loss: 2.1439 - posture_output_loss: 0.1878 - subject_output_categorical_accuracy: 0.2704 - posture_output_categorical_accuracy: 0.9162 - val_loss: 2.0396 - val_subject_output_loss: 1.9595 - val_posture_output_loss: 0.0800 - val_subject_output_categorical_accuracy: 0.2754 - val_posture_output_categorical_accuracy: 0.9948\n",
      "Epoch 37/40\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 2.2745 - subject_output_loss: 2.0125 - posture_output_loss: 0.2619 - subject_output_categorical_accuracy: 0.2808 - posture_output_categorical_accuracy: 0.9175 - val_loss: 2.0036 - val_subject_output_loss: 1.8622 - val_posture_output_loss: 0.1415 - val_subject_output_categorical_accuracy: 0.3964 - val_posture_output_categorical_accuracy: 0.9943\n",
      "Epoch 38/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.2999 - subject_output_loss: 2.0026 - posture_output_loss: 0.2973 - subject_output_categorical_accuracy: 0.2795 - posture_output_categorical_accuracy: 0.9153 - val_loss: 2.1129 - val_subject_output_loss: 1.9487 - val_posture_output_loss: 0.1642 - val_subject_output_categorical_accuracy: 0.3239 - val_posture_output_categorical_accuracy: 0.9957\n",
      "Epoch 39/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.2091 - subject_output_loss: 2.0842 - posture_output_loss: 0.1249 - subject_output_categorical_accuracy: 0.2843 - posture_output_categorical_accuracy: 0.9288 - val_loss: 2.0181 - val_subject_output_loss: 1.9097 - val_posture_output_loss: 0.1084 - val_subject_output_categorical_accuracy: 0.3590 - val_posture_output_categorical_accuracy: 0.9915\n",
      "Epoch 40/40\n",
      "147/147 [==============================] - 1s 8ms/step - loss: 2.2926 - subject_output_loss: 2.1031 - posture_output_loss: 0.1895 - subject_output_categorical_accuracy: 0.3027 - posture_output_categorical_accuracy: 0.9124 - val_loss: 1.9037 - val_subject_output_loss: 1.8254 - val_posture_output_loss: 0.0783 - val_subject_output_categorical_accuracy: 0.4196 - val_posture_output_categorical_accuracy: 0.9984\n",
      "Fold 0:\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 11:09:03.489761: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inbase_cnn_model/dropout_12/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/147 [============================>.] - ETA: 0s - loss: 5.3300 - subject_output_loss: 3.5766 - posture_output_loss: 1.7534 - subject_output_categorical_accuracy: 0.1071 - posture_output_categorical_accuracy: 0.4728"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 64\n",
    "regularization = 0\n",
    "\n",
    "performances = {}\n",
    "for lambd in [0.2, 0.5, 0.6, 0.8]:\n",
    "    performances[lambd] = []\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "        print(f\"Fold {i}:\")\n",
    "        base_cnn_model=create_model(regularization)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "        base_cnn_model.compile(optimizer = optimizer,\n",
    "                loss=custom_loss(lambd),\n",
    "                metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "        history = base_cnn_model.fit(\n",
    "            X[train_index], \n",
    "            {\n",
    "                'subject_output': label_s_oh[train_index], \n",
    "                'posture_output': y_oh[train_index]\n",
    "            },\n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size, \n",
    "            validation_data=(\n",
    "                X[test_index], \n",
    "                {\n",
    "                    'subject_output': label_s_oh[test_index], \n",
    "                    'posture_output': y_oh[test_index]\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        performances[lambd].append([history.history[\"val_subject_output_categorical_accuracy\"], history.history[\"val_posture_output_categorical_accuracy\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.41100690364837644, 0.65595383644104, 0.3922888934612274]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean performances for each lamda on subject [0] or posture [1] accuracy\n",
    "[np.mean(np.max([el[0] for el in perf], axis=1)) for perf in performances.values()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
